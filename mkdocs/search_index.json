{
    "docs": [
        {
            "location": "/", 
            "text": "GalaxyKickstarter\n\n\nGalaxyKickstarter is an \nAnsible\n playbook designed for installing, testing, deploying and \nmaintaining production-grade Galaxy instances.\n\nIn the basic configuration, this includes:\n\n\n\n\npostgresql server as database backend \n\n\nnginx proxy \n\n\nslurm cluster\n\n\n\n\nIn adition, tools and workflows can be managed.", 
            "title": "Home"
        }, 
        {
            "location": "/#galaxykickstarter", 
            "text": "GalaxyKickstarter is an  Ansible  playbook designed for installing, testing, deploying and \nmaintaining production-grade Galaxy instances. \nIn the basic configuration, this includes:   postgresql server as database backend   nginx proxy   slurm cluster   In adition, tools and workflows can be managed.", 
            "title": "GalaxyKickstarter"
        }, 
        {
            "location": "/about/", 
            "text": "GalaxyKickstarter\n\n\nGalaxyKickstarter is an Ansible playbook designed to help you get one or more production-ready\n \nGalaxy servers\n based on Ubuntu within minutes, and to maintain these servers.\n\n\nOptionally, instances can be pre-loaded with tools and workflows.\n\n\nThe playbook has been tested on \n\n\n\n\nCloud Machines\n\n\nVagrant Boxes\n\n\nPhysical Servers \n\n\nDocker.\n\n\n\n\nGalaxyKickstarter has been developed at the \nARTbio platform\n and contains roles developed\nby the \nGalaxy team\n.\n\n\nList of roles included in this playbook\n\n\nansible-postgresql-objects\n\n\ngalaxy-extras role\n\n\ngalaxy-tools role\n\n\ngalaxy-os role\n\n\ngalaxy role", 
            "title": "What is GalaxyKickstarter"
        }, 
        {
            "location": "/about/#galaxykickstarter", 
            "text": "GalaxyKickstarter is an Ansible playbook designed to help you get one or more production-ready\n  Galaxy servers  based on Ubuntu within minutes, and to maintain these servers.  Optionally, instances can be pre-loaded with tools and workflows.  The playbook has been tested on    Cloud Machines  Vagrant Boxes  Physical Servers   Docker.   GalaxyKickstarter has been developed at the  ARTbio platform  and contains roles developed\nby the  Galaxy team .", 
            "title": "GalaxyKickstarter"
        }, 
        {
            "location": "/about/#list-of-roles-included-in-this-playbook", 
            "text": "ansible-postgresql-objects  galaxy-extras role  galaxy-tools role  galaxy-os role  galaxy role", 
            "title": "List of roles included in this playbook"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Getting Started\n\n\nGetting the playbook\n\n\nGalaxyKickstarter is hosted on \ngithub\n and makes use of submodules, so care\nneeds to be taken to also download the submodules. Cloning the repository for the first time can be done like this\n(note the \n--recursive\n):\n\n\ngit clone --recursive https://github.com/ARTbio/ansible-artimed.git\n\n\n\n\nThe playbook (here \ngalaxy.yml\n) should be in the ansible-artimed folder.\n\n\nls ansible-artimed/\nCONTRIBUTORS.md  docs  extra-files  galaxy.yml  group_vars  hosts\nLICENSE.txt  mkdocs.yml  pre-commit.sh  README.md  roles  Vagrantfile\n\n\n\n\nDeploying galaxy-kickstart on remote machines.\n\n\n\n\nInside the repository you will find a hosts file.\nThis is an example inventory.\n\n\n[artimed]\nlocalhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n...\n\n\n\n\nHere \n[artimed]\n is a group, that contains a machine called localhost.\nThe variables defined in \ngroup_vars/artimed\n will be applied to this host.\nAnsible will connect by ssh to this machine, using the ssh key in \n~/.ssh/id_rsa\n.\n\n\nIf you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine),\ncreate a new inventory, and change \nlocalhost\n to the IP address of that machine.\n\nansible_ssh_user=\nuser\n controls under which username to connect to this machine.\nThis user needs to have sudo rights.\n\n\nThen, run the plabook by typing:\n\n\nansible-playbook --inventory-file=\nyour_inventory\n galaxy.yml\n\n\n\n\nYou can put multiple machines in your inventory.\nIf you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped.\nWhenever you change a variable (see \ncustomizations\n), you need to run the playbook again.", 
            "title": "Getting started"
        }, 
        {
            "location": "/getting_started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#getting-the-playbook", 
            "text": "GalaxyKickstarter is hosted on  github  and makes use of submodules, so care\nneeds to be taken to also download the submodules. Cloning the repository for the first time can be done like this\n(note the  --recursive ):  git clone --recursive https://github.com/ARTbio/ansible-artimed.git  The playbook (here  galaxy.yml ) should be in the ansible-artimed folder.  ls ansible-artimed/\nCONTRIBUTORS.md  docs  extra-files  galaxy.yml  group_vars  hosts\nLICENSE.txt  mkdocs.yml  pre-commit.sh  README.md  roles  Vagrantfile", 
            "title": "Getting the playbook"
        }, 
        {
            "location": "/getting_started/#deploying-galaxy-kickstart-on-remote-machines", 
            "text": "Inside the repository you will find a hosts file.\nThis is an example inventory.  [artimed]\nlocalhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n...  Here  [artimed]  is a group, that contains a machine called localhost.\nThe variables defined in  group_vars/artimed  will be applied to this host.\nAnsible will connect by ssh to this machine, using the ssh key in  ~/.ssh/id_rsa .  If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine),\ncreate a new inventory, and change  localhost  to the IP address of that machine. ansible_ssh_user= user  controls under which username to connect to this machine.\nThis user needs to have sudo rights.  Then, run the plabook by typing:  ansible-playbook --inventory-file= your_inventory  galaxy.yml  You can put multiple machines in your inventory.\nIf you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped.\nWhenever you change a variable (see  customizations ), you need to run the playbook again.", 
            "title": "Deploying galaxy-kickstart on remote machines."
        }, 
        {
            "location": "/customizations/", 
            "text": "Customising the playbook\n\n\nWe strongly encourage users to read the \nansible inventory\n documentation first.\n\n\nMost settings should be editable without modifying the playbook directly,\ninstead variables can be set in group_vars and host vars.\n\n\nThe playbook comes with an example inventory file \nhosts\n.\n\n\n[artimed]\nlocalhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n[travis_bioblend]\nlocalhost ansible_connection=local\n[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].\n\n\n\n\n[artimed]\n, \n[travis_bioblend]\n and \n[aws]\n are predefined groups. Any host (here we only have localhost) that\nis added to one or multiple groups will have the corresponding group variables applied.\nGroup variables are defined in \ngroup_vars/[name of the group]\n and default variables are found in \n\n\ngroup_vars/all\n.\nAll variables defined in \ngroup_vars/all\n are overwritten in \ngroup_vars/[name of the group]\n.  \n\n\nFor instance the variable \nproftpd_nat_masquerade\n is set to \nfalse\n in \ngroup_vars/all\n, while hosts in the \n[aws]\n group\napply the \n[aws]\n group variables which set \nproftpd_nat_masquerade\n to true, so that hosts in the aws group will have\nthis aws-specific setting applied. Any combination of groups may be used.\n\n\nIf you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that\nthe host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains\nthe group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost,\nyour inventory file should look like this:\n\n\n[metavisitor]\nlocalhost\n\n\n\n\nYou can then run the playbook as usual:\n\n\nansible-playbook --inventory-file=\nyour_inventory_file\n galaxy.yml\n\n\n\n\nImportant variables\n\n\nWe aimed for this playbook to be reusable. We therefore made most variables configurable.\nThe group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file\nor you can use ansible group variables to selectively set the variables for certain hosts/groups. See the \nansible documentation\nabout group variables\n for details.\n\n\nThese most important variables are:\n\n\n\n\n\n\nansible_ssh_user - The login name used to access the target.\n\n\n\n\n\n\nansible_ssh_private_key_file - The ssh private key used to access the target.\n\n\n\n\n\n\ninstall_galaxy - True for install a Galaxy instance.\n\n\n\n\n\n\ninstall_tools - True for install the NGS tools.\n\n\n\n\n\n\nrun_data_manager - True for run the data manager procedure.\n\n\n\n\n\n\ngalaxy_user_name - The Operating System user name for galaxy process.\n\n\n\n\n\n\ngalaxy_server_dir - The home of Operating System user for galaxy process.\n\n\n\n\n\n\ngalaxy_admin - The admin galaxy user.\n\n\n\n\n\n\ngalaxy_admin_pw - The admin galaxy password.\n\n\n\n\n\n\ndefault_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production.\n\n\n\n\n\n\ngalaxy_tool_list - The files that constants the list of tools to be installed.\n\n\n\n\n\n\ngalaxy_data_managers - The reference genomes and indexes to be load and build.\n\n\n\n\n\n\ngalaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered.\n\n\n\n\n\n\ngalaxy_database - The persistent directory where postgresql will be installed or will be recovered.\n\n\n\n\n\n\ngalaxy_db - Connection string for galaxy-postgresql.\n\n\n\n\n\n\ngalaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).", 
            "title": "Customizations"
        }, 
        {
            "location": "/customizations/#customising-the-playbook", 
            "text": "We strongly encourage users to read the  ansible inventory  documentation first.  Most settings should be editable without modifying the playbook directly,\ninstead variables can be set in group_vars and host vars.  The playbook comes with an example inventory file  hosts .  [artimed]\nlocalhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n[travis_bioblend]\nlocalhost ansible_connection=local\n[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].  [artimed] ,  [travis_bioblend]  and  [aws]  are predefined groups. Any host (here we only have localhost) that\nis added to one or multiple groups will have the corresponding group variables applied.\nGroup variables are defined in  group_vars/[name of the group]  and default variables are found in   group_vars/all .\nAll variables defined in  group_vars/all  are overwritten in  group_vars/[name of the group] .    For instance the variable  proftpd_nat_masquerade  is set to  false  in  group_vars/all , while hosts in the  [aws]  group\napply the  [aws]  group variables which set  proftpd_nat_masquerade  to true, so that hosts in the aws group will have\nthis aws-specific setting applied. Any combination of groups may be used.  If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that\nthe host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains\nthe group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost,\nyour inventory file should look like this:  [metavisitor]\nlocalhost  You can then run the playbook as usual:  ansible-playbook --inventory-file= your_inventory_file  galaxy.yml", 
            "title": "Customising the playbook"
        }, 
        {
            "location": "/customizations/#important-variables", 
            "text": "We aimed for this playbook to be reusable. We therefore made most variables configurable.\nThe group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file\nor you can use ansible group variables to selectively set the variables for certain hosts/groups. See the  ansible documentation\nabout group variables  for details.  These most important variables are:    ansible_ssh_user - The login name used to access the target.    ansible_ssh_private_key_file - The ssh private key used to access the target.    install_galaxy - True for install a Galaxy instance.    install_tools - True for install the NGS tools.    run_data_manager - True for run the data manager procedure.    galaxy_user_name - The Operating System user name for galaxy process.    galaxy_server_dir - The home of Operating System user for galaxy process.    galaxy_admin - The admin galaxy user.    galaxy_admin_pw - The admin galaxy password.    default_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production.    galaxy_tool_list - The files that constants the list of tools to be installed.    galaxy_data_managers - The reference genomes and indexes to be load and build.    galaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered.    galaxy_database - The persistent directory where postgresql will be installed or will be recovered.    galaxy_db - Connection string for galaxy-postgresql.    galaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).", 
            "title": "Important variables"
        }, 
        {
            "location": "/installing tools and workflows/", 
            "text": "Installing tools\n\n\n\n\nThis playbook includes the \nansible-galaxy-tools\n role which can be used\nto install tools and workflows into galaxy instances using the \nbioblend\n api.  \n\n\nCreating a tool_list.yml file\n\n\nTo install tools, you will need to prepare a list of tools in yaml format.\nA an example of a a tool list can be found in \nhere\n\n\ntools:\n- name: blast_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: blastx_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: bowtie2\n  owner: devteam\n  revisions:\n  - 019c2a81547a\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n\n\n\n\nwhen the revision is empty, the latest available revision will be installed.\n\ntool_panel_section_label will determine the tool panel section where the tools will be found.\n\n\nObtaining a tool_list.yml file\n\n\nWe can also obtain a tool list from a runnning galaxy instance.\nNote that for server running a galaxy release \n16.04, you need a galaxy API keys and bioblend.\nA script is included in the extra-files directory.\n\n\npython get_tool_yml_from_gi.py --galaxy \nmy_galaxy_url\n --api-key \nmy_admin_api_key\n --output-file \nmy_tool_list.yml\n\n\n\n\n\nAdding a tool_list.yml file to a group_variable files\n\n\nGroup variable files are in the group_vars directory.\n\n\nIf you would like to install tools, you need to reference the tool_list.yml in the group variable file.\nWe typically place additional files in the \nextra-files/\nhostname\n/\nhostname\n_tool_list.yml\n file.\n\n\nIf you would like to add tools to a group that is called metavisitor edit \ngroup_vars/metavisitor\n and add these lines:\n\n\ninstall_tools: true\ngalaxy_tools_tool_list: \nextra-files/metavisitor/metavisitor_tool_list.yml\n\n\n\n\n\nInstalling workflows\n\n\nYou can also make sure that workflows are available after running the playbook.\nAs with tools, place the workflows in \nextra-files/\nhostname\n/\nhostname\nworkflow_name\n.ga\n\nAdd these lines to the corresponding group_var file:\n\n\ngalaxy_tools_install_workflows: true\ngalaxy_tools_workflows:\n  - \nextra-files/metavisitor/Galaxy-Workflow-create_model.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga\n\n\n\n\n\nRunning the playbook\n\n\nAs per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group.\nIf the target is localhost, your inventory file should look ike this:\n\n\n[metavisitor]\nlocalhost\n\n\n\n\nthen run the playbook like so:\n\n\nansible-playbook --inventory-file=\nyour_inventory_file\n galaxy.yml", 
            "title": "Installing tools and workflows"
        }, 
        {
            "location": "/installing tools and workflows/#installing-tools", 
            "text": "This playbook includes the  ansible-galaxy-tools  role which can be used\nto install tools and workflows into galaxy instances using the  bioblend  api.", 
            "title": "Installing tools"
        }, 
        {
            "location": "/installing tools and workflows/#creating-a-tool_listyml-file", 
            "text": "To install tools, you will need to prepare a list of tools in yaml format.\nA an example of a a tool list can be found in  here  tools:\n- name: blast_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: blastx_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: bowtie2\n  owner: devteam\n  revisions:\n  - 019c2a81547a\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/  when the revision is empty, the latest available revision will be installed. \ntool_panel_section_label will determine the tool panel section where the tools will be found.", 
            "title": "Creating a tool_list.yml file"
        }, 
        {
            "location": "/installing tools and workflows/#obtaining-a-tool_listyml-file", 
            "text": "We can also obtain a tool list from a runnning galaxy instance.\nNote that for server running a galaxy release  16.04, you need a galaxy API keys and bioblend.\nA script is included in the extra-files directory.  python get_tool_yml_from_gi.py --galaxy  my_galaxy_url  --api-key  my_admin_api_key  --output-file  my_tool_list.yml", 
            "title": "Obtaining a tool_list.yml file"
        }, 
        {
            "location": "/installing tools and workflows/#adding-a-tool_listyml-file-to-a-group_variable-files", 
            "text": "Group variable files are in the group_vars directory.  If you would like to install tools, you need to reference the tool_list.yml in the group variable file.\nWe typically place additional files in the  extra-files/ hostname / hostname _tool_list.yml  file.  If you would like to add tools to a group that is called metavisitor edit  group_vars/metavisitor  and add these lines:  install_tools: true\ngalaxy_tools_tool_list:  extra-files/metavisitor/metavisitor_tool_list.yml", 
            "title": "Adding a tool_list.yml file to a group_variable files"
        }, 
        {
            "location": "/installing tools and workflows/#installing-workflows", 
            "text": "You can also make sure that workflows are available after running the playbook.\nAs with tools, place the workflows in  extra-files/ hostname / hostname workflow_name .ga \nAdd these lines to the corresponding group_var file:  galaxy_tools_install_workflows: true\ngalaxy_tools_workflows:\n  -  extra-files/metavisitor/Galaxy-Workflow-create_model.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga", 
            "title": "Installing workflows"
        }, 
        {
            "location": "/installing tools and workflows/#running-the-playbook", 
            "text": "As per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group.\nIf the target is localhost, your inventory file should look ike this:  [metavisitor]\nlocalhost  then run the playbook like so:  ansible-playbook --inventory-file= your_inventory_file  galaxy.yml", 
            "title": "Running the playbook"
        }, 
        {
            "location": "/examples/docker/", 
            "text": "Building and deploying galaxy-kickstart in docker\n\n\n\n\nRequirements\n\n\nYou need to have docker installed and configured for your user.\n\n\nThe repository comes with various Dockerfiles that can be used to configure a deployment using Docker,\nor you can start with a pre-built docker image.\n\n\nRunning images from the dockerhub\n\n\nYou can obtain a pre-built docker image from the dockerhub:\n\n\ndocker pull artbio/galaxy-kickstart-base\n\n\n\n\nStart the image and serve it on port 8080 of your local machine in the standard docker way:\n\n\nCID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base`\n\n\n\n\n-p 8080:80\n will forward requests to nginx inside the container running on port 80.\n\n\nRuntime changes to pre-built docker images\n\n\nIf you wish to reach the container on a subdirectory, add \n-e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"\n to the docker call \nand galaxy will be served at \nhttp://127.0.0.1:8080/my-subdirectory\n.\n\n\nWe recommend changing the default admin user as well, so the command becomes:\n\n\nCID=`docker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`\n\n\n\n\nCommit changed containers to new images\n\n\nAs with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking:\nCommit the changes to my-new-image\n\n\ndocker commit $CID my-new-image\n\n\n\n\n\nStop and remove the original container:\n\n\ndocker stop $CID \n docker rm $CID\n\n\n\n\nStart the new container:\n\n\nCID=`docker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`\n\n\n\n\nPersisting to disk\n\n\nAll changes made to the container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nAssuming you would like to mount your local \n/data\n folder, run\n\n\nCID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image`\n\n\n\n\nThis will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Docker"
        }, 
        {
            "location": "/examples/docker/#building-and-deploying-galaxy-kickstart-in-docker", 
            "text": "", 
            "title": "Building and deploying galaxy-kickstart in docker"
        }, 
        {
            "location": "/examples/docker/#requirements", 
            "text": "You need to have docker installed and configured for your user.  The repository comes with various Dockerfiles that can be used to configure a deployment using Docker,\nor you can start with a pre-built docker image.", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/docker/#running-images-from-the-dockerhub", 
            "text": "You can obtain a pre-built docker image from the dockerhub:  docker pull artbio/galaxy-kickstart-base  Start the image and serve it on port 8080 of your local machine in the standard docker way:  CID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base`  -p 8080:80  will forward requests to nginx inside the container running on port 80.", 
            "title": "Running images from the dockerhub"
        }, 
        {
            "location": "/examples/docker/#runtime-changes-to-pre-built-docker-images", 
            "text": "If you wish to reach the container on a subdirectory, add  -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"  to the docker call \nand galaxy will be served at  http://127.0.0.1:8080/my-subdirectory .  We recommend changing the default admin user as well, so the command becomes:  CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`", 
            "title": "Runtime changes to pre-built docker images"
        }, 
        {
            "location": "/examples/docker/#commit-changed-containers-to-new-images", 
            "text": "As with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking:\nCommit the changes to my-new-image  docker commit $CID my-new-image  Stop and remove the original container:  docker stop $CID   docker rm $CID  Start the new container:  CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`", 
            "title": "Commit changed containers to new images"
        }, 
        {
            "location": "/examples/docker/#persisting-to-disk", 
            "text": "All changes made to the container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nAssuming you would like to mount your local  /data  folder, run  CID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image`  This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Persisting to disk"
        }, 
        {
            "location": "/examples/vagrant/", 
            "text": "Deploying galaxy-kickstart on local virtual machine (VM) using vagrant.\n\n\n\n\nGalaxyKickstarter is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box\nthat runs this playbook. Following these instructions will not change the host system.\nAlternatively, see \nexamples/docker\n for running the playbook in docker,\nor \ngetting started\n for running the playbook on local or remote machines.\n\n\nRequirements\n\n\nTo follow the examples \nansible\n, \nvagrant\n\nand \ngit\n need to be installed.\n\n\nRunning the playbook on a Virtual Machine\n\n\nThe Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty).\n\n\nVAGRANTFILE_API_VERSION = \n2\n\n   Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n      config.vm.box = \nubuntu/trusty64\n\n      config.vm.network \nforwarded_port\n, guest: 80, host: 8080\n      config.vm.network \nforwarded_port\n, guest: 21, host: 2121\n\n      config.vm.provider \nvirtualbox\n do |v|\n         v.memory = 4096\n      end\n\n      config.vm.provision \nansible\n do |ansible|\n         ansible.extra_vars = {\n            ntp_server: \npool.ntp.org\n,\n            ansible_ssh_user: 'vagrant'\n         }\n         ansible.verbose = 'vvvv'\n         ansible.playbook = \ngalaxy.yml\n\n      end\n   end\n\n\n\n\nBy default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP),\nand 4096 MB of memory will be attributed to the VM.\nEnter the playbook directory \ncd ansible-artimed\n and type \nvagrant up\n to download a VM image and run the \ngalaxy.yml\n playbook.\n\n\nThis will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 .\nIf you would like to see the internals of the VM, you can log into the machine by typing \nvagrant ssh\n.\n\n\nvagrant up\n makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner\nand running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222)\nThe hosts inventory file contains an example for directly pointing ansible to the vagrant machine.\nUncomment the vagrant specific lines and comment or remove the remaining lines:\n\n\n#[artimed]\n#localhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n#[travis_bioblend]\n#localhost ansible_connection=local\n# Uncomment the 2 lines below to point ansible to a local vagrant machine.\n[all]\nlocalhost ansible_user=\nvagrant\n ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key\n#[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].\n\n\n\n\nTo run the playbook again, type\n\n\nansible-playbook --inventory-file=\nyour_inventory\n galaxy.yml\n\n\n\n\nCleaning up\n\n\nThe VM image and various config files have been written to the \n.vagrant\n folder. Type \nvagrant halt\n to stop the running instance\nand \nvagrant destroy\n to remove the VM, and then delete the \n.vagrant\n folder.", 
            "title": "Vagrant"
        }, 
        {
            "location": "/examples/vagrant/#deploying-galaxy-kickstart-on-local-virtual-machine-vm-using-vagrant", 
            "text": "GalaxyKickstarter is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box\nthat runs this playbook. Following these instructions will not change the host system.\nAlternatively, see  examples/docker  for running the playbook in docker,\nor  getting started  for running the playbook on local or remote machines.", 
            "title": "Deploying galaxy-kickstart on local virtual machine (VM) using vagrant."
        }, 
        {
            "location": "/examples/vagrant/#requirements", 
            "text": "To follow the examples  ansible ,  vagrant \nand  git  need to be installed.", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/vagrant/#running-the-playbook-on-a-virtual-machine", 
            "text": "The Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty).  VAGRANTFILE_API_VERSION =  2 \n   Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n      config.vm.box =  ubuntu/trusty64 \n      config.vm.network  forwarded_port , guest: 80, host: 8080\n      config.vm.network  forwarded_port , guest: 21, host: 2121\n\n      config.vm.provider  virtualbox  do |v|\n         v.memory = 4096\n      end\n\n      config.vm.provision  ansible  do |ansible|\n         ansible.extra_vars = {\n            ntp_server:  pool.ntp.org ,\n            ansible_ssh_user: 'vagrant'\n         }\n         ansible.verbose = 'vvvv'\n         ansible.playbook =  galaxy.yml \n      end\n   end  By default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP),\nand 4096 MB of memory will be attributed to the VM.\nEnter the playbook directory  cd ansible-artimed  and type  vagrant up  to download a VM image and run the  galaxy.yml  playbook.  This will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 .\nIf you would like to see the internals of the VM, you can log into the machine by typing  vagrant ssh .  vagrant up  makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner\nand running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222)\nThe hosts inventory file contains an example for directly pointing ansible to the vagrant machine.\nUncomment the vagrant specific lines and comment or remove the remaining lines:  #[artimed]\n#localhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n#[travis_bioblend]\n#localhost ansible_connection=local\n# Uncomment the 2 lines below to point ansible to a local vagrant machine.\n[all]\nlocalhost ansible_user= vagrant  ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key\n#[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].  To run the playbook again, type  ansible-playbook --inventory-file= your_inventory  galaxy.yml", 
            "title": "Running the playbook on a Virtual Machine"
        }, 
        {
            "location": "/examples/vagrant/#cleaning-up", 
            "text": "The VM image and various config files have been written to the  .vagrant  folder. Type  vagrant halt  to stop the running instance\nand  vagrant destroy  to remove the VM, and then delete the  .vagrant  folder.", 
            "title": "Cleaning up"
        }, 
        {
            "location": "/faq/", 
            "text": "What is the username and password of the galaxy admin account ?\n\n\nUsername and password of the galaxy account are controlled by the variables \ngalaxy_admin\n and \ngalaxy_admin_pw\n and\ndefault to \nadmin@galaxy.org\n and \nadmin\n (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on.\nIf you have a host in the \nmygroup\n group, you can edit group_vars/my_group and set\n\n\ngalaxy_admin: new_admin@email.com\ngalaxy_admin_pw: new_password\n\n\n\n\nAs with each change, run the playbook again.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/faq/#what-is-the-username-and-password-of-the-galaxy-admin-account", 
            "text": "Username and password of the galaxy account are controlled by the variables  galaxy_admin  and  galaxy_admin_pw  and\ndefault to  admin@galaxy.org  and  admin  (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on.\nIf you have a host in the  mygroup  group, you can edit group_vars/my_group and set  galaxy_admin: new_admin@email.com\ngalaxy_admin_pw: new_password  As with each change, run the playbook again.", 
            "title": "What is the username and password of the galaxy admin account ?"
        }, 
        {
            "location": "/available_roles/", 
            "text": "ansible-postgresql-objects\n\n\ngalaxy-extras role\n\n\ngalaxy-tools role\n\n\ngalaxy-os role\n\n\ngalaxy role", 
            "title": "Available roles"
        }, 
        {
            "location": "/available_variables/", 
            "text": "", 
            "title": "Available variables"
        }, 
        {
            "location": "/editing_help/", 
            "text": "To view the help, type\n\n\nmkdocs serve\n\n\n\n\nThis will serve the documentation on http://127.0.0.1:8000.\n\n\nTo add new files, create a markdown document in docs/, and reference it in mkdocs.yml.\n\nIf you want to add a document called \nediting_help.md\n add the following line.\n\n\n - Editing the readme: editing_help.md\n\n\n\n\nso that \nmkdocs.yml\n looks like this.\n\n\nsite_name: GalaxyKickstarter\npages:\n - Home: index.md\n - What is GalaxyKickstarter: about.md\n - Getting started: getting_started.md\n - Customizations: customizations.md\n - Examples: examples.md\n - Available roles: available_roles.md\n - Available variables: available_variables.md\n - Editing the readme: editing_help.md\ntheme: readthedocs", 
            "title": "Editing the readme"
        }, 
        {
            "location": "/about_metavisitor/", 
            "text": "Metavisitor\n\n\nMetavisitor\n is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules.\n\n\nHere we first provide  documentation on how to install and use a Galaxy server instances with pre-installed Metavisitor tools and workflows.\n\n\n\n\nUsing GalaxyKickstarter: see \nMetavisitor with GalaxyKickstarter (Ansible)\n\n\nUsing Docker: see \nMetavitor with Docker\n\n\n\n\nUsers who have already the Metavisitor suite of tools installed in their own instance, or who just want to use it on the Galaxy Mississippi Server\n\n\ncan skip these chapters and go directly to the chapter \nPrepare input data histories\n and following.\n\n\nMetavisitor has been developed at the \nARTbio platform\n its tools and workflows are available in [GitHub] (https://github.com/ARTbio/tools-artbio) and in the \nGalaxy main tool shed\n.", 
            "title": "About metavisitor"
        }, 
        {
            "location": "/about_metavisitor/#metavisitor", 
            "text": "Metavisitor  is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules.  Here we first provide  documentation on how to install and use a Galaxy server instances with pre-installed Metavisitor tools and workflows.   Using GalaxyKickstarter: see  Metavisitor with GalaxyKickstarter (Ansible)  Using Docker: see  Metavitor with Docker", 
            "title": "Metavisitor"
        }, 
        {
            "location": "/about_metavisitor/#users-who-have-already-the-metavisitor-suite-of-tools-installed-in-their-own-instance-or-who-just-want-to-use-it-on-the-galaxy-mississippi-server", 
            "text": "can skip these chapters and go directly to the chapter  Prepare input data histories  and following.  Metavisitor has been developed at the  ARTbio platform  its tools and workflows are available in [GitHub] (https://github.com/ARTbio/tools-artbio) and in the  Galaxy main tool shed .", 
            "title": "Users who have already the Metavisitor suite of tools installed in their own instance, or who just want to use it on the Galaxy Mississippi Server"
        }, 
        {
            "location": "/metavisitor_ansible/", 
            "text": "Installing Metavisitor with GalaxyKickstarter and Ansible\n\n\n\n\nHere, a \nDeployment Machine\n will install a Metavisitor Galaxy server on \nTarget Machine\n. Note that \nDeployment Machine\n and \nTarget Machine\n can both be local or remote machines, and that they can be the same machine.\n\n\nRequirements\n\n\n\n\n\n\nOn the \nDeployment Machine\n, \ngit\n and \nansible\n need to be installed.\n\n\nThe \nTarget Machine\n has to be accessible through ssh connection by the user (you) with \nroot\n privileges. This implies that a correct ssh private key file is available on your \nDeployment Machine\n, for instance \n~/.ssh/id_rsa\n. This key will be used for secure transactions managed by ansible between the \nDeployment Machine\n and the \nTarget Machine\n.\n\n\n\n\nGetting the ansible playbook\n\n\n\n\nThis is done on the \nDeployment Machine\n by cloning the \nGalaxyKickstarter (ansible-artimed) repository\n hosted by \nthe ARTbio organization\n:\n\n\nIn your terminal, type:\n\n\ngit clone --recursive https://github.com/ARTbio/ansible-artimed.git\n\n\n\n\nImportantly, GalaxyKickstarter makes use of submodules, so care\nneeds to be taken to also download these submodules. This is why \n--recursive\n is included in the git command line.\n\n\nAt completion of the git cloning, you will have a new \nansible-artimed\n folder, which contains everything need for deployment with ansible, including the playbook file (here \ngalaxy.yml\n). You can verify this by typing in terminal:\n\n\nls -la ansible-artimed\n\n\nAdapting the ansible-artimed folder to your deployment\n\n\n\n\nThere are only few things to change in the \nansible-artimed\n folder before running ansible.\n\n\nAdapt the ansible inventory file\n\n\nIn the ansible-artimed folder, there is a \nhosts\n file called the \"inventory file\".\nFor deploying Metavisitor, you need to edit this file so that it just contains\n\n\n\n[metavisitor]\n\n\nip address\n ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\npath/to/the/ssh/private/key\n\n\n\n\n\n\nThe \nip address\n is the address of the \nTarget Machine\n. The \npath/to/the/ssh/private/key\n is the path \non the \nDeployment Machine\n to your ssh key, to be recognized by the \nTarget Machine\n.\n\n\nThus, a practical exemple of the final content on the inventory file \nhosts\n is:\n\n\n\n[metavisitor]\n\n192.54.201.126 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n\n\n\n\n\nwhere \n192.54.201.126\n is the ip address of the \nTarget machine\n and \n~/.ssh/id_rsa\n the path to the private ssh key.\n\n\nAdapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine\n\n\nIn this specific case, add in the hosts inventory file:\n\n\n[metavisitor]\n192.54.201.126 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/aws_private_key.pem\n\n\n[aws]\n192.54.201.126\n\n\n\n\nIn that case \naws_private_key.pem\n is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud.\n\n\nNote that in addition the settings of the security group associated to the AWS instance should be as follows:\n\n\nType            |Protocole|   Port Range  |  Source   | #comment\n__________________________________________________________________________________________\nHTTP            |   TCP   |      80       | 0.0.0.0/0 | for Galaxy web access\nSSH             |   TCP   |      22       | 0.0.0.0/0 | for ssh access to the AWS instance\nCustom TCP Rule |   TCP   |      21       | 0.0.0.0/0 | for FTP upload to Galaxy\nCustom TCP Rule |   TCP   | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy\n\n\n\n\nThe ports 21 and  49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy.\n\n\nAdapt the group_vars/all file for persisting data, if needed.\n\n\nIn cases where your \nTarget machine\n has volumes where you wish the Galaxy data to be persisted in, you have to edit the \nansible-artimed/group_vars/all\n file, to indicate the path to this volume on the \nTarget machine\n.\n\n\nIf you don't understand the previous statement, no worries, just don't do anything and skip this step.\n\n\nFor others, find the lines\n\n\n#persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export\n\n\n\n\nin the \nansible-artimed/group_vars/all\n file, and change /export to the path of your persistent volume.\n\n\nNote that if \n/export\n is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the \n/home/galaxy/galaxy\n folder of the \nTarget Machine\n.\n\n\nDeploying Metavisitor Galaxy on the \nTarget Machine\n\n\n\n\nYou are almost done.\n\n\nNavigate with your terminal to your \nansible-artimed\n folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the \nTarget Machine\n:\n\n\nansible-playbook --inventory-file=hosts galaxy.yml\n\n\n\n\nIf everything is ok, you may be asked to authorize the access to the \nTarget Machine\n by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the \nTarget Machine\n in this terminal.\n\n\nWhen the process is finished, you should be able to access the \nTarget Machine\n by typing its IP address in your web browser.\n\n\nBy default the admin login/password is \nadmin@galaxy.org\n / \nadmin\n. You should change the password for safety.\n\n\nRe-deploying Metavisitor Galaxy on the \nTarget Machine\n\n\n\n\nIf you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickstarter by editing the content of \nansible-artimed\n.\n\n\nIn that case, when your changes are done, just run again the command\n\n\nansible-playbook --inventory-file=hosts galaxy.yml\n\n\n\n\nWhen you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped.\nWhenever you change a variable (see \ncustomizations\n), you need to run the playbook again.\n\n\nYou can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses:\n\n\n[metavisitor]\n192.54.201.126 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n192.54.201.127 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n192.54.201.128 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa", 
            "title": "Installing with GalaxyKickstarter"
        }, 
        {
            "location": "/metavisitor_ansible/#installing-metavisitor-with-galaxykickstarter-and-ansible", 
            "text": "Here, a  Deployment Machine  will install a Metavisitor Galaxy server on  Target Machine . Note that  Deployment Machine  and  Target Machine  can both be local or remote machines, and that they can be the same machine.", 
            "title": "Installing Metavisitor with GalaxyKickstarter and Ansible"
        }, 
        {
            "location": "/metavisitor_ansible/#requirements", 
            "text": "On the  Deployment Machine ,  git  and  ansible  need to be installed.  The  Target Machine  has to be accessible through ssh connection by the user (you) with  root  privileges. This implies that a correct ssh private key file is available on your  Deployment Machine , for instance  ~/.ssh/id_rsa . This key will be used for secure transactions managed by ansible between the  Deployment Machine  and the  Target Machine .", 
            "title": "Requirements"
        }, 
        {
            "location": "/metavisitor_ansible/#getting-the-ansible-playbook", 
            "text": "This is done on the  Deployment Machine  by cloning the  GalaxyKickstarter (ansible-artimed) repository  hosted by  the ARTbio organization :  In your terminal, type:  git clone --recursive https://github.com/ARTbio/ansible-artimed.git  Importantly, GalaxyKickstarter makes use of submodules, so care\nneeds to be taken to also download these submodules. This is why  --recursive  is included in the git command line.  At completion of the git cloning, you will have a new  ansible-artimed  folder, which contains everything need for deployment with ansible, including the playbook file (here  galaxy.yml ). You can verify this by typing in terminal:  ls -la ansible-artimed", 
            "title": "Getting the ansible playbook"
        }, 
        {
            "location": "/metavisitor_ansible/#adapting-the-ansible-artimed-folder-to-your-deployment", 
            "text": "There are only few things to change in the  ansible-artimed  folder before running ansible.", 
            "title": "Adapting the ansible-artimed folder to your deployment"
        }, 
        {
            "location": "/metavisitor_ansible/#adapt-the-ansible-inventory-file", 
            "text": "In the ansible-artimed folder, there is a  hosts  file called the \"inventory file\".\nFor deploying Metavisitor, you need to edit this file so that it just contains  \n[metavisitor] ip address  ansible_ssh_user= root  ansible_ssh_private_key_file= path/to/the/ssh/private/key   The  ip address  is the address of the  Target Machine . The  path/to/the/ssh/private/key  is the path  on the  Deployment Machine  to your ssh key, to be recognized by the  Target Machine .  Thus, a practical exemple of the final content on the inventory file  hosts  is:  \n[metavisitor]\n\n192.54.201.126 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa   where  192.54.201.126  is the ip address of the  Target machine  and  ~/.ssh/id_rsa  the path to the private ssh key.", 
            "title": "Adapt the ansible inventory file"
        }, 
        {
            "location": "/metavisitor_ansible/#adapt-the-ansible-inventory-file-to-an-amazon-web-service-aws-virtual-machine", 
            "text": "In this specific case, add in the hosts inventory file:  [metavisitor]\n192.54.201.126 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/aws_private_key.pem \n\n[aws]\n192.54.201.126  In that case  aws_private_key.pem  is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud.  Note that in addition the settings of the security group associated to the AWS instance should be as follows:  Type            |Protocole|   Port Range  |  Source   | #comment\n__________________________________________________________________________________________\nHTTP            |   TCP   |      80       | 0.0.0.0/0 | for Galaxy web access\nSSH             |   TCP   |      22       | 0.0.0.0/0 | for ssh access to the AWS instance\nCustom TCP Rule |   TCP   |      21       | 0.0.0.0/0 | for FTP upload to Galaxy\nCustom TCP Rule |   TCP   | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy  The ports 21 and  49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy.", 
            "title": "Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine"
        }, 
        {
            "location": "/metavisitor_ansible/#adapt-the-group_varsall-file-for-persisting-data-if-needed", 
            "text": "In cases where your  Target machine  has volumes where you wish the Galaxy data to be persisted in, you have to edit the  ansible-artimed/group_vars/all  file, to indicate the path to this volume on the  Target machine .  If you don't understand the previous statement, no worries, just don't do anything and skip this step.  For others, find the lines  #persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export  in the  ansible-artimed/group_vars/all  file, and change /export to the path of your persistent volume.  Note that if  /export  is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the  /home/galaxy/galaxy  folder of the  Target Machine .", 
            "title": "Adapt the group_vars/all file for persisting data, if needed."
        }, 
        {
            "location": "/metavisitor_ansible/#deploying-metavisitor-galaxy-on-the-target-machine", 
            "text": "You are almost done.  Navigate with your terminal to your  ansible-artimed  folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the  Target Machine :  ansible-playbook --inventory-file=hosts galaxy.yml  If everything is ok, you may be asked to authorize the access to the  Target Machine  by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the  Target Machine  in this terminal.  When the process is finished, you should be able to access the  Target Machine  by typing its IP address in your web browser.  By default the admin login/password is  admin@galaxy.org  /  admin . You should change the password for safety.", 
            "title": "Deploying Metavisitor Galaxy on the Target Machine"
        }, 
        {
            "location": "/metavisitor_ansible/#re-deploying-metavisitor-galaxy-on-the-target-machine", 
            "text": "If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickstarter by editing the content of  ansible-artimed .  In that case, when your changes are done, just run again the command  ansible-playbook --inventory-file=hosts galaxy.yml  When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped.\nWhenever you change a variable (see  customizations ), you need to run the playbook again.  You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses:  [metavisitor]\n192.54.201.126 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n192.54.201.127 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n192.54.201.128 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa", 
            "title": "Re-deploying Metavisitor Galaxy on the Target Machine"
        }, 
        {
            "location": "/metavisitor_docker/", 
            "text": "Installing Metavisitor with Docker\n\n\n\n\nWe distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".\n\n\nRequirements\n\n\nYou need to have docker installed and configured for your user.\n\n\nRunning images from the dockerhub\n\n\nYou can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container:\n\n\ndocker search metavisitor\n\n\n\n\nThen, to get the docker image, type:\n\n\ndocker pull artbio/metavisitor-1.2\n\n\n\n\nIn this documentation, we recommend to use the \nartbio/metavisitor-1.2\n which better corresponds to the environment described in our \nMetavisitor preprint\n\n\nWhen this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing:\n\n\ndocker run -d -p 80:80 artbio/metavisitor-1.2\n\n\n\n\nThis command starts a container in daemon mode (\n-d\n) from the image and serve it on port 80 of the local machine in the standard docker way.\n\n\n-p 80:80\n forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change \n-p 80:80\n to \n-p 8080:80\n\n\nRuntime changes to pre-built docker images\n\n\nIf you wish to reach the container on a subdirectory, add \n-e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"\n to the docker call.\n\n\nFor instance,\n\n\ndocker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -p 80:80 artbio/metavisitor-1.2\n\n\n\n\nwill get the metavisitor docker container serving at \nhttp://127.0.0.1:80/my-subdirectory\n.\n\n\nWe recommend also changing the default admin user as well, so the command becomes:\n\n\ndocker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base\n\n\n\n\nNote that is you do not make this latest change, the admin login for the metavisitor container is by default \nadmin@galaxy.org\n and the password is \nadmin\n.\n\n\nPersisting to disk\n\n\nAll changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nThus, assuming you would like to mount your local \n/my/data\n folder and persist you Galaxy data in this folder, run\n\n\ndocker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-1.2\n\n\n\n\nThis will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Installing with Docker"
        }, 
        {
            "location": "/metavisitor_docker/#installing-metavisitor-with-docker", 
            "text": "We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".", 
            "title": "Installing Metavisitor with Docker"
        }, 
        {
            "location": "/metavisitor_docker/#requirements", 
            "text": "You need to have docker installed and configured for your user.", 
            "title": "Requirements"
        }, 
        {
            "location": "/metavisitor_docker/#running-images-from-the-dockerhub", 
            "text": "You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container:  docker search metavisitor  Then, to get the docker image, type:  docker pull artbio/metavisitor-1.2  In this documentation, we recommend to use the  artbio/metavisitor-1.2  which better corresponds to the environment described in our  Metavisitor preprint  When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing:  docker run -d -p 80:80 artbio/metavisitor-1.2  This command starts a container in daemon mode ( -d ) from the image and serve it on port 80 of the local machine in the standard docker way.  -p 80:80  forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change  -p 80:80  to  -p 8080:80", 
            "title": "Running images from the dockerhub"
        }, 
        {
            "location": "/metavisitor_docker/#runtime-changes-to-pre-built-docker-images", 
            "text": "If you wish to reach the container on a subdirectory, add  -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"  to the docker call.  For instance,  docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -p 80:80 artbio/metavisitor-1.2  will get the metavisitor docker container serving at  http://127.0.0.1:80/my-subdirectory .  We recommend also changing the default admin user as well, so the command becomes:  docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base  Note that is you do not make this latest change, the admin login for the metavisitor container is by default  admin@galaxy.org  and the password is  admin .", 
            "title": "Runtime changes to pre-built docker images"
        }, 
        {
            "location": "/metavisitor_docker/#persisting-to-disk", 
            "text": "All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nThus, assuming you would like to mount your local  /my/data  folder and persist you Galaxy data in this folder, run  docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-1.2  This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Persisting to disk"
        }, 
        {
            "location": "/metavisitor_access_control/", 
            "text": "When you are done with the installation of your own Metavisitor Galaxy instance installation using either \nGalaxykickstarter\n or \ndocker\n, there are a few basic things to know for web access and basic server admin operations\n\n\n1. Connect web frontpage of your Metavisitor Galaxy\n\n\nWe assume that you know the IP address to reach the Metavisitor Galaxy webserver:\n\n\n\n\nif you used \nGalaxykickstarter\n, you had to indicate this host IP in your hosts inventory file.\n\n\nif you used \ndocker\n, you had to connect to the host machine with the appropriate IP address.\n\n\n\n\nThus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser.\n\n\nIf you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it.\n\n\nIn case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this \n/subdirectory\n in your url which then looks like \nhttp://\nIP\n/subdirectory\n\n\n2. Log to the Galaxy server using the admin credentials:\n\n\nIf everything goes well, you should now see the Galaxy Metavisitor home page in your web browser.\n\n\n\n\nYou have to log as the admin.\n\n\n\n\nTo do that, go to the User menu and click \nlogin\n\n\n\n\nThis is your first login, thus the admin login is \nadmin@galaxy.org\n and your admin password is \nadmin\n\n\nHowever\n for security, immediately change the admin password. To do this, go again in the Users menu, \nPreferences\n\n\n\n\nAnd click the \nChange your password\n item in the User preferences.\n\n\nBasic admin operation: restart the Metavisitor Galaxy instance\n\n\nAs we will see in the \nnext chapter\n, installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it.\n\n\nrestart Metavisitor Galaxy instance deployed with \nGalaxykickstarter\n\n\n\n\n\n\nConnect to the server where the Galaxy instance has been installed either through the ssh connection you have used with \nGalaxykickstarter\n\n\n\n\n\n\nin your terminal, type \nsudo supervisorctl restart galaxy:\n\n\n\n\n\n\nIf everything went fine you should see in your terminal\n\n\n# supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started\n\n\n\n\nThat's it, the Galaxy instance has restarted.\n\n\nrestart Metavisitor Galaxy instance deployed with \ndocker\n\n\n\n\n\n\nConnect to the server where the Galaxy instance has been installed either through the ssh connection you have used with \nGalaxykickstarter\n\n\n\n\n\n\nconnect to you docker host using ssh\n\n\n\n\ntype \ndocker ps\n. You should see your Metavisitor docker container running and the name of the container in the NAMES column\n\n\nenter into your container by typing:\n\n\n\n\ndocker exec -it \nname_of_the_container\n bash\n\n- in the docker session you can now type \nsudo supervisorctl restart galaxy:\n\n\nand see also, within the container:\n\n\n# supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started\n\n\n\n\nThat's it, the Galaxy instance has restarted. you can leave the container by typing \nexit", 
            "title": "Access and Control Metavisitor Galaxy instance"
        }, 
        {
            "location": "/metavisitor_access_control/#1-connect-web-frontpage-of-your-metavisitor-galaxy", 
            "text": "We assume that you know the IP address to reach the Metavisitor Galaxy webserver:   if you used  Galaxykickstarter , you had to indicate this host IP in your hosts inventory file.  if you used  docker , you had to connect to the host machine with the appropriate IP address.   Thus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser.  If you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it.  In case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this  /subdirectory  in your url which then looks like  http:// IP /subdirectory", 
            "title": "1. Connect web frontpage of your Metavisitor Galaxy"
        }, 
        {
            "location": "/metavisitor_access_control/#2-log-to-the-galaxy-server-using-the-admin-credentials", 
            "text": "If everything goes well, you should now see the Galaxy Metavisitor home page in your web browser.   You have to log as the admin.   To do that, go to the User menu and click  login   This is your first login, thus the admin login is  admin@galaxy.org  and your admin password is  admin  However  for security, immediately change the admin password. To do this, go again in the Users menu,  Preferences   And click the  Change your password  item in the User preferences.", 
            "title": "2. Log to the Galaxy server using the admin credentials:"
        }, 
        {
            "location": "/metavisitor_access_control/#basic-admin-operation-restart-the-metavisitor-galaxy-instance", 
            "text": "As we will see in the  next chapter , installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it.", 
            "title": "Basic admin operation: restart the Metavisitor Galaxy instance"
        }, 
        {
            "location": "/metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-galaxykickstarter", 
            "text": "Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with  Galaxykickstarter    in your terminal, type  sudo supervisorctl restart galaxy:    If everything went fine you should see in your terminal  # supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started  That's it, the Galaxy instance has restarted.", 
            "title": "restart Metavisitor Galaxy instance deployed with Galaxykickstarter"
        }, 
        {
            "location": "/metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-docker", 
            "text": "Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with  Galaxykickstarter    connect to you docker host using ssh   type  docker ps . You should see your Metavisitor docker container running and the name of the container in the NAMES column  enter into your container by typing:   docker exec -it  name_of_the_container  bash \n- in the docker session you can now type  sudo supervisorctl restart galaxy:  and see also, within the container:  # supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started  That's it, the Galaxy instance has restarted. you can leave the container by typing  exit", 
            "title": "restart Metavisitor Galaxy instance deployed with docker"
        }, 
        {
            "location": "/metavisitor_configure_references/", 
            "text": "Once you know how to access to your Metavisitor Galaxy instance with a web browser and are able to perform basic start/stop/restart operations, there is still some work needed to import and configure reference data (genomes) so that they are directly available to all instance users for running tools and workflows\n\n\nHere we provide the step-by-step description of what we \nactually did ourselves\n to prepare our Metavisitor instance before performing the analyses described here \nhere\n.\n\n\n1. Connect to your Metavisitor Galaxy admin account with your web browser\n\n\n2. Import reference data in an history \"References\"\n\n\nAt first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library.\n\n\na. Preliminary actions\n\n\nclick on the \nAnalyze Data\n menu\nrename the \nUnnamed history\n to \nReferences\n\n\nb. Upload nucleotide vir1 fasta file\n\n\nClick on the small arrow icon at the top of the tool bar (left handside of the Galaxy interface)\nIn the open window, click on the Paste/Fetch data button\nPaste the URL https://ndownloader.figshare.com/files/4949173\nClick the start button.\n\n\n\n\nA first dataset will show up in your history, first in grey (the job is starting), then yellow (the job - upload - is running), and eventually green (job is successfully done).\n\n\nWhen finished, rename the dataset 1 \nhttps://ndownloader.figshare.com/files/4949173\n to \nnucleotide vir1\n for clarity (using the small pencil icon).\n\n\nc. Upload protein vir1 fasta file\n\n\nRepeat the same operation as in \nb.\n using the url \nhttps://ndownloader.figshare.com/files/4949170\n.\nWhen finished, rename the dataset 2 \nhttps://ndownloader.figshare.com/files/4949170\n to \nprotein vir1\n for clarity.\n\n\nd. Upload the Drosophila melanogaster release 6 genome\n\n\nRepeat the same operation as in \nb.\n using the url \nftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz\n.\nWhen finished, rename the dataset 3 \nftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz\n to \ndm6\n for clarity.\n\n\ne. Upload the Anopheles gambiae release P4\n\n\nRepeat the same operation as in \nb.\n using the url \nhttps://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz\n.\nWhen finished, rename the dataset 4 \nhttps://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz\n to \nAgamP4\n for clarity.\n\n\nf. Upload the Plasmodium berghei genome\n\n\nRepeat the same operation as in \nb.\n using the url \nftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz\n.\nWhen finished, rename the dataset 5 \nftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz\n to \nP. berghei\n for clarity.\n\n\ng. Upload the Homo sapiens release GRCh38/hg19\n\n\nRepeat the same operation as in \nb.\n using the url \nftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n.\nWhen finished, rename the dataset 6 \nftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n to \nhg19\n for clarity.\n\n\n3. Prepare Blast databases\n\n\na. Nucleotide vir1 blast database\n\n\nUse the tool \nNCBI BLAST+ makeblastdb\n, check the radio button \"nucleotide\", select the dataset 1 (nucleotide vir1), give \"nucleotide vir1 blastdb\" as a \"Title for BLAST database\", leave the rest of the tool form unchanged and click \"Execute\" button.\n\n\nRename the generated dataset 7 \"nucleotide BLAST database from data 1\" to \"nucleotide vir1 blast database\" for clarity\n\n\nb. Protein vir1 blast database\n\n\nUse the tool \nNCBI BLAST+ makeblastdb\n, check the radio button \"protein\", select the dataset 2 (protein vir1), give \"protein vir1 blastdb\" as a \"Title for BLAST database\", leave the rest of the tool form unchanged and click \"Execute\" button.\n\n\nRename the generated dataset 8 \"protein BLAST database from data 2\" to \"protein vir1 blast database\" for clarity\n\n\n4. Creating Galaxy dbkey and fasta references accessible to tools for every user\n\n\nHere we are going in the \nadmin\n panel, click \nLocal data\n in the left menu and select the \nCreate DBKey and Reference Genome\n in the \"\nRun Data Manager Tools\n\" (last line of the top section).\n\n\na. nucleotide vir1\n\n\nin the newly open browser window (from the last click on \nCreate DBKey and Reference Genome\n)\n- select \"New\" for \nUse existing dbkey or create a new one\n\n- enter \"vir1\" in the \ndbkey\nfield\n- leave \"Display name for dbkey\", \nName of sequence\n and \nID for sequence\n empty !\n- select \nhistory\n in the \nChoose the source for the reference genome\n menu\n- select \"nucleotide vir1\" in the \nFASTA File\n menu\n- let \nSort by chromosome name\n selected on \nAs is\n\n- press the \nexecute\nbutton !\n\n\nb. dm6\n\n\nRepeat the operation described in a., but this time\n\n\n\n\nput \"dm6\" for the \ndbkey\n field\n\n\nselect \"3: dm6\" in the \nFASTA File\n menu\n\n\n\n\nBe sure that the \nReferences\n history is selected in the background, otherwise the uploaded genomes will not be available in this menu.\n\n\nc. AgamP4\n\n\nRepeat the operation described in a., but this time\n\n\n\n\nput \"AgamP4\" for the \ndbkey\n field\n\n\nselect \"4: AgamP4\" in the \nFASTA File\n menu\n\n\n\n\nBe sure that the \nReferences\n history is selected in the background, otherwise the uploaded genomes will not be available in this menu.\n\n\nd. hg19\n\n\nRepeat the operation described in a., but this time\n\n\n\n\nput \"hg19\" for the \ndbkey\n field\n\n\nselect \"6: hg19\" in the \nFASTA File\n menu\n\n\n\n\nBe sure that the \nReferences\n history is selected in the background, otherwise the uploaded genomes will not be available in this menu.\n\n\n5. Creating Galaxy bowtie indexes accessible to tools for every user\n\n\nNow we are going to generate the bowtie indexes using another data manager tool.\nBut before doing this, we have to perform a low level Galaxy admin task: restart the Galaxy server instance so that the dbkey and the fasta genomes that we've just created for the server are registered and seen by the tools.\n\n\nDepending on your skill level, you have the simple, dirty way:\n\n\n\n\nreboot the machine where the galaxy server is running !\n\n\n\n\nor the clean, freaking (for non-Geek normal biologists) way:\n\n\n\n\nConnect to the server where the Galaxy instance has been installed either through an ssh connection or using your local terminal. and type\n\n\n\n\nsudo supervisorctl restart galaxy:\n\n\nIf everything went fine you should see in your terminal\n\n\n# supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started\n\n\n\n\nfreaking insn't it ?\n\n\n\n\na. vir 1 bowtie index\n\n\nNow, Go back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"vir1\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\nb. dm6 bowtie index\n\n\nGo back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"dm6\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\nc. AgamP4 bowtie index\n\n\nGo back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"AgamP4\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\nd. hg19 bowtie index\n\n\nGo back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"hg19\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\n\n\nNote that the preparation of bowtie indexes can be long ! (several hours for the vir1 bowtie index for instance)\n\n\n When bowtie indexes are ready (green in the \nData Manager History (automatically created)\n) restart the Galaxy server instance as explained above, so that the bowtie indexes that we've just created for the server are registered and seen by the tools.\n\n\n6. Creating Galaxy bowtie2 indexes accessible to tools for every user\n\n\nFinally, we are going to generate the bowtie2 indexes using another data manager tool.\nIf not done before, restart the Galaxy instance as explained above.\n\n\n\n\na. vir 1 bowtie2 index\n\n\nNow, Go back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie2 index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"vir1\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\nb. AgamP4 bowtie2 index\n\n\nGo back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie2 index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"AgamP4\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\nc. hg19 bowtie2 index\n\n\nGo back to your web browser and the \nadmin\n panel, click again \nLocal data\n in the left menu and  select this time the \nBowtie2 index - builder\n in the \"\nRun Data Manager Tools\n\" (top section).\n\n\nselect \"hg19\" in the \nSource FASTA Sequence\n menu of the Bowtie index builder tool form, leave the other options empty, and click execute.\n\n\n\n\nNote that the preparation of bowtie2 indexes can be long too ! (several hours for the vir1 bowtie2 index for instance)\n\n\n When bowtie2 indexes are ready (green in the \nData Manager History (automatically created)\n) restart the Galaxy server instance as explained above, so that the bowtie indexes that we've just created for the server are registered and seen by the tools.", 
            "title": "Prepare Metavisitor Galaxy instance for analyses"
        }, 
        {
            "location": "/metavisitor_configure_references/#1-connect-to-your-metavisitor-galaxy-admin-account-with-your-web-browser", 
            "text": "", 
            "title": "1. Connect to your Metavisitor Galaxy admin account with your web browser"
        }, 
        {
            "location": "/metavisitor_configure_references/#2-import-reference-data-in-an-history-references", 
            "text": "At first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library.", 
            "title": "2. Import reference data in an history \"References\""
        }, 
        {
            "location": "/metavisitor_configure_references/#a-preliminary-actions", 
            "text": "click on the  Analyze Data  menu\nrename the  Unnamed history  to  References", 
            "title": "a. Preliminary actions"
        }, 
        {
            "location": "/metavisitor_configure_references/#b-upload-nucleotide-vir1-fasta-file", 
            "text": "Click on the small arrow icon at the top of the tool bar (left handside of the Galaxy interface)\nIn the open window, click on the Paste/Fetch data button\nPaste the URL https://ndownloader.figshare.com/files/4949173\nClick the start button.  A first dataset will show up in your history, first in grey (the job is starting), then yellow (the job - upload - is running), and eventually green (job is successfully done).  When finished, rename the dataset 1  https://ndownloader.figshare.com/files/4949173  to  nucleotide vir1  for clarity (using the small pencil icon).", 
            "title": "b. Upload nucleotide vir1 fasta file"
        }, 
        {
            "location": "/metavisitor_configure_references/#c-upload-protein-vir1-fasta-file", 
            "text": "Repeat the same operation as in  b.  using the url  https://ndownloader.figshare.com/files/4949170 .\nWhen finished, rename the dataset 2  https://ndownloader.figshare.com/files/4949170  to  protein vir1  for clarity.", 
            "title": "c. Upload protein vir1 fasta file"
        }, 
        {
            "location": "/metavisitor_configure_references/#d-upload-the-drosophila-melanogaster-release-6-genome", 
            "text": "Repeat the same operation as in  b.  using the url  ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz .\nWhen finished, rename the dataset 3  ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz  to  dm6  for clarity.", 
            "title": "d. Upload the Drosophila melanogaster release 6 genome"
        }, 
        {
            "location": "/metavisitor_configure_references/#e-upload-the-anopheles-gambiae-release-p4", 
            "text": "Repeat the same operation as in  b.  using the url  https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz .\nWhen finished, rename the dataset 4  https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz  to  AgamP4  for clarity.", 
            "title": "e. Upload the Anopheles gambiae release P4"
        }, 
        {
            "location": "/metavisitor_configure_references/#f-upload-the-plasmodium-berghei-genome", 
            "text": "Repeat the same operation as in  b.  using the url  ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz .\nWhen finished, rename the dataset 5  ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz  to  P. berghei  for clarity.", 
            "title": "f. Upload the Plasmodium berghei genome"
        }, 
        {
            "location": "/metavisitor_configure_references/#g-upload-the-homo-sapiens-release-grch38hg19", 
            "text": "Repeat the same operation as in  b.  using the url  ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz .\nWhen finished, rename the dataset 6  ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz  to  hg19  for clarity.", 
            "title": "g. Upload the Homo sapiens release GRCh38/hg19"
        }, 
        {
            "location": "/metavisitor_configure_references/#3-prepare-blast-databases", 
            "text": "", 
            "title": "3. Prepare Blast databases"
        }, 
        {
            "location": "/metavisitor_configure_references/#a-nucleotide-vir1-blast-database", 
            "text": "Use the tool  NCBI BLAST+ makeblastdb , check the radio button \"nucleotide\", select the dataset 1 (nucleotide vir1), give \"nucleotide vir1 blastdb\" as a \"Title for BLAST database\", leave the rest of the tool form unchanged and click \"Execute\" button.  Rename the generated dataset 7 \"nucleotide BLAST database from data 1\" to \"nucleotide vir1 blast database\" for clarity", 
            "title": "a. Nucleotide vir1 blast database"
        }, 
        {
            "location": "/metavisitor_configure_references/#b-protein-vir1-blast-database", 
            "text": "Use the tool  NCBI BLAST+ makeblastdb , check the radio button \"protein\", select the dataset 2 (protein vir1), give \"protein vir1 blastdb\" as a \"Title for BLAST database\", leave the rest of the tool form unchanged and click \"Execute\" button.  Rename the generated dataset 8 \"protein BLAST database from data 2\" to \"protein vir1 blast database\" for clarity", 
            "title": "b. Protein vir1 blast database"
        }, 
        {
            "location": "/metavisitor_configure_references/#4-creating-galaxy-dbkey-and-fasta-references-accessible-to-tools-for-every-user", 
            "text": "Here we are going in the  admin  panel, click  Local data  in the left menu and select the  Create DBKey and Reference Genome  in the \" Run Data Manager Tools \" (last line of the top section).", 
            "title": "4. Creating Galaxy dbkey and fasta references accessible to tools for every user"
        }, 
        {
            "location": "/metavisitor_configure_references/#a-nucleotide-vir1", 
            "text": "in the newly open browser window (from the last click on  Create DBKey and Reference Genome )\n- select \"New\" for  Use existing dbkey or create a new one \n- enter \"vir1\" in the  dbkey field\n- leave \"Display name for dbkey\",  Name of sequence  and  ID for sequence  empty !\n- select  history  in the  Choose the source for the reference genome  menu\n- select \"nucleotide vir1\" in the  FASTA File  menu\n- let  Sort by chromosome name  selected on  As is \n- press the  execute button !", 
            "title": "a. nucleotide vir1"
        }, 
        {
            "location": "/metavisitor_configure_references/#b-dm6", 
            "text": "Repeat the operation described in a., but this time   put \"dm6\" for the  dbkey  field  select \"3: dm6\" in the  FASTA File  menu   Be sure that the  References  history is selected in the background, otherwise the uploaded genomes will not be available in this menu.", 
            "title": "b. dm6"
        }, 
        {
            "location": "/metavisitor_configure_references/#c-agamp4", 
            "text": "Repeat the operation described in a., but this time   put \"AgamP4\" for the  dbkey  field  select \"4: AgamP4\" in the  FASTA File  menu   Be sure that the  References  history is selected in the background, otherwise the uploaded genomes will not be available in this menu.", 
            "title": "c. AgamP4"
        }, 
        {
            "location": "/metavisitor_configure_references/#d-hg19", 
            "text": "Repeat the operation described in a., but this time   put \"hg19\" for the  dbkey  field  select \"6: hg19\" in the  FASTA File  menu   Be sure that the  References  history is selected in the background, otherwise the uploaded genomes will not be available in this menu.", 
            "title": "d. hg19"
        }, 
        {
            "location": "/metavisitor_configure_references/#5-creating-galaxy-bowtie-indexes-accessible-to-tools-for-every-user", 
            "text": "Now we are going to generate the bowtie indexes using another data manager tool.\nBut before doing this, we have to perform a low level Galaxy admin task: restart the Galaxy server instance so that the dbkey and the fasta genomes that we've just created for the server are registered and seen by the tools.  Depending on your skill level, you have the simple, dirty way:   reboot the machine where the galaxy server is running !   or the clean, freaking (for non-Geek normal biologists) way:   Connect to the server where the Galaxy instance has been installed either through an ssh connection or using your local terminal. and type   sudo supervisorctl restart galaxy:  If everything went fine you should see in your terminal  # supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started  freaking insn't it ?", 
            "title": "5. Creating Galaxy bowtie indexes accessible to tools for every user"
        }, 
        {
            "location": "/metavisitor_configure_references/#a-vir-1-bowtie-index", 
            "text": "Now, Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie index - builder  in the \" Run Data Manager Tools \" (top section).  select \"vir1\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "a. vir 1 bowtie index"
        }, 
        {
            "location": "/metavisitor_configure_references/#b-dm6-bowtie-index", 
            "text": "Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie index - builder  in the \" Run Data Manager Tools \" (top section).  select \"dm6\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "b. dm6 bowtie index"
        }, 
        {
            "location": "/metavisitor_configure_references/#c-agamp4-bowtie-index", 
            "text": "Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie index - builder  in the \" Run Data Manager Tools \" (top section).  select \"AgamP4\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "c. AgamP4 bowtie index"
        }, 
        {
            "location": "/metavisitor_configure_references/#d-hg19-bowtie-index", 
            "text": "Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie index - builder  in the \" Run Data Manager Tools \" (top section).  select \"hg19\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "d. hg19 bowtie index"
        }, 
        {
            "location": "/metavisitor_configure_references/#note-that-the-preparation-of-bowtie-indexes-can-be-long-several-hours-for-the-vir1-bowtie-index-for-instance", 
            "text": "When bowtie indexes are ready (green in the  Data Manager History (automatically created) ) restart the Galaxy server instance as explained above, so that the bowtie indexes that we've just created for the server are registered and seen by the tools.", 
            "title": "Note that the preparation of bowtie indexes can be long ! (several hours for the vir1 bowtie index for instance)"
        }, 
        {
            "location": "/metavisitor_configure_references/#6-creating-galaxy-bowtie2-indexes-accessible-to-tools-for-every-user", 
            "text": "Finally, we are going to generate the bowtie2 indexes using another data manager tool.\nIf not done before, restart the Galaxy instance as explained above.", 
            "title": "6. Creating Galaxy bowtie2 indexes accessible to tools for every user"
        }, 
        {
            "location": "/metavisitor_configure_references/#a-vir-1-bowtie2-index", 
            "text": "Now, Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie2 index - builder  in the \" Run Data Manager Tools \" (top section).  select \"vir1\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "a. vir 1 bowtie2 index"
        }, 
        {
            "location": "/metavisitor_configure_references/#b-agamp4-bowtie2-index", 
            "text": "Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie2 index - builder  in the \" Run Data Manager Tools \" (top section).  select \"AgamP4\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "b. AgamP4 bowtie2 index"
        }, 
        {
            "location": "/metavisitor_configure_references/#c-hg19-bowtie2-index", 
            "text": "Go back to your web browser and the  admin  panel, click again  Local data  in the left menu and  select this time the  Bowtie2 index - builder  in the \" Run Data Manager Tools \" (top section).  select \"hg19\" in the  Source FASTA Sequence  menu of the Bowtie index builder tool form, leave the other options empty, and click execute.", 
            "title": "c. hg19 bowtie2 index"
        }, 
        {
            "location": "/metavisitor_configure_references/#note-that-the-preparation-of-bowtie2-indexes-can-be-long-too-several-hours-for-the-vir1-bowtie2-index-for-instance", 
            "text": "When bowtie2 indexes are ready (green in the  Data Manager History (automatically created) ) restart the Galaxy server instance as explained above, so that the bowtie indexes that we've just created for the server are registered and seen by the tools.", 
            "title": "Note that the preparation of bowtie2 indexes can be long too ! (several hours for the vir1 bowtie2 index for instance)"
        }, 
        {
            "location": "/use_cases_input_data/", 
            "text": "We are now entering into real analyses using Metavisitor.\nThese analyses as well as their biological context are are presented as \nUse Cases\n in the \nmetavisitor article\n. We invite readers of this manual to refer to this article if they need to better understand the biological context of the described procedures.\n\n\nIn this section, we are going to create step by step a Galaxy history that contains the input data required to run the workflows for Use Cases 1-1, 1-2, 1-3 and 1-4\n\n\nHistory with input data for Use Cases 1-1, 1-2, 1-3 and 1-4\n\n\n\n\nCreate a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\"\n\n\nimport SRP013822 datasets\n\n\nUse the tool \nExtract reads in FASTQ/A format from NCBI SRA\n and fill the SRR accession field with the first EBI SRA identifier \nSRR515090\n\n\nrepeat the exact same operation with the tool \nExtract reads in FASTQ/A format from NCBI SRA\n and the identifiers \nSRR513993, SRR513992, SRR513990, SRR513989, SRR513981, SRR513901\n\n\nfor the 7 datasets retrieved from EBI SRA, change the datatype \nfastq\n to \nfastqsanger\n:\nclick on the pencil icon of the dataset, click the tab Datatype, and select fastqsanger in the New Type menu\n\n\n\n\n\n\n\n\nCreate a dataset collection \nSRP013822\n\n\n\n\nClick on the checked box icon at the top of history bar as indicated below\n\n\n\n\n\n\n\n\nSelect the 7 datasets\n\n\nSelect \nBuild Dataset List\n in the menu \nFor all selected...\n\n\n\n\n\n\n\n\nand type \nSRP013822\n in the Name field and click \nCreate list\n\n\nyou can leave the checked datasets view by clicking again the check box in the history top menu\n\n\n\n\n\n\n\n\ncopy the vir1 blast database that we have prepared earlier in the \nReference\n history.\n\n\n\n\nTo do so, click on the little wheel icon in the history top menu (in the history right bar).\n\n\n\n\n\n\n\n\nSelect \"Copy Datasets\"\n\n\nIn the open page, select \"References\" in the Source History menu, check the \"nucleotide vir1 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\".\n\n\nif you refresh the history, you will see the \"nucleotide vir1 blast database\" dataset showing up.\n\n\n\n\n\n\n\n\nThat is all for the moment. We will latter add datasets in the history \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n. However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !", 
            "title": "Prepare input data histories"
        }, 
        {
            "location": "/use_cases_input_data/#history-with-input-data-for-use-cases-1-1-1-2-1-3-and-1-4", 
            "text": "Create a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\"  import SRP013822 datasets  Use the tool  Extract reads in FASTQ/A format from NCBI SRA  and fill the SRR accession field with the first EBI SRA identifier  SRR515090  repeat the exact same operation with the tool  Extract reads in FASTQ/A format from NCBI SRA  and the identifiers  SRR513993, SRR513992, SRR513990, SRR513989, SRR513981, SRR513901  for the 7 datasets retrieved from EBI SRA, change the datatype  fastq  to  fastqsanger :\nclick on the pencil icon of the dataset, click the tab Datatype, and select fastqsanger in the New Type menu     Create a dataset collection  SRP013822   Click on the checked box icon at the top of history bar as indicated below     Select the 7 datasets  Select  Build Dataset List  in the menu  For all selected...     and type  SRP013822  in the Name field and click  Create list  you can leave the checked datasets view by clicking again the check box in the history top menu     copy the vir1 blast database that we have prepared earlier in the  Reference  history.   To do so, click on the little wheel icon in the history top menu (in the history right bar).     Select \"Copy Datasets\"  In the open page, select \"References\" in the Source History menu, check the \"nucleotide vir1 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\".  if you refresh the history, you will see the \"nucleotide vir1 blast database\" dataset showing up.     That is all for the moment. We will latter add datasets in the history  Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 . However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !", 
            "title": "History with input data for Use Cases 1-1, 1-2, 1-3 and 1-4"
        }, 
        {
            "location": "/use_case_1/", 
            "text": "Histories for Use Cases 1-1, 1-2, 1-3 and 1-4\n\n\nAs you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start \nfrom\n the \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history, run the appropriate workflow, \nsending the outputs of the workflow in a new history\n named accordingly.\n\n\nHistory for Use Case 1-1.\n\n\n1. As aforementioned, ensure that you are in the \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history.\n\n\nYou can always control this by using the top menu \nUsers\n --\n \nSaved History\n and selecting the desired history. If you don't see the History right bar, just click in addition the top menu \nAnalyze Data\n\n\n2. Select the appropriate workflow\n\n\n\n\nClick now on the \nWorkflow\n top menu\n\n\nSelect the workflow \n\"Metavisitor: Workflow for Use Case 1-1 (imported from API)\"\n and to see the workflow, select the submenu \n\"Edit\"\n\n\n\n\nNow that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting \n\"Run\"\n\n\n\n\n\n\n\n\n3. Select the appropriate parameters before running the workflow\n\n\nYou now see a page with all the workflow steps, whose top part looks like:\n\n\n\n\n\n\n\n\nAs pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the \nncbi_guide_ID\n. In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the \nncbi_guide_ID\n field the \nNC_007919.3_\nvalue. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets.\n\n\n\n\n\n\nYou have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the \nprevious chapter\n), there is no other option in the menu than \"\nSRP013822\n\".\n\n\n\n\n\n\nYou have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \"\nnucleotide vir1 blast database\n\", just because there is only one dataset in the input history with the \"blast database\" type.\n\n\n\n\n\n\nYou can review the other steps of the workflow. But there is no other selection to perform before running the workflow.\n\n\n\n\n\n\n4. Running the workflow \nsending the outputs in a new history\n\n\nWe are almost ready, but before clicking the \"\nRun Workflow\n\" button there is an important thing to do:\n- Check the \"\nSend results to a new history\n\" checkbox as shown Here\n\n\n\n\n\n\nAnd edit the field to \"History for Use Case 1-1\"\n\n\nYou can now click the \"Run workflow\" button.\n\nThis trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history.\n\n\n\n\nWhen the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see:\n\n\n\n\nNote that 24 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets\n\n\nHistories for Use Cases 1-2 and 1-3\n\n\nHistories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1.\n\n\nDo exactly as described for Use Case 1-1 and\n- Remember to go back to the \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history and be sure you are going to run the workflow from that history.\n- \nSelect the appropriate workflow\n !\n- Remember to Check the \"\nSend results to a new history\n\" checkbox, rename the new history appropriately before pressing the \"\nRun workflow\n\" button\n\n\nHistory for remapping in Use Cases 1-1,2,3\n\n\nBefore running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history.\n\n\nupdate the \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history\n\n\nThis is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408).\n\n\nThus, go back to the \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history and\n\n\n\n\nUse the \nRetrieve FASTA from NCBI\n Metavisitor tool to retrieve the NC_007919.3 sequence.\n\n\nUse the \nRegex Find And Replace\n tool on the \nRetrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3'\n dataset as an input, and put \ngi\\|346421290\\|ref\\|NC_007919.3\\|_Nora_virus,_complete_genome\n as Find Regex parameter and \nNC_007919.3\n as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity.\n\n\nUse the \nRetrieve FASTA from NCBI\n Metavisitor tool to retrieve the JX220408 sequence.\n\n\nUse the \nRegex Find And Replace\n tool on the \nRetrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408'\n dataset as an input, and put \ngi\\|402295620\\|gb\\|JX220408.1\\|_Nora_virus_isolate_FR1,_complete_genome\n as Find Regex parameter and \nJX220408.1\n as Replacement parameter. Rename the generated dataset JX220408.1 for clarity.\n\n\nclick on the top wheel history icon, select \nCopy Datasets\n; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\".\n\n\nRepeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset.\n\n\nand Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset.\n\n\nyou may have to refresh your \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n history to reveal the three copied datasets\n\n\nThe last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the \nFor all selected...\n button, select \nBuild Dataset List\n, name this list \"Nora virus genomes\", and press the \nCreate list\n button.\n\n\n\n\nWe are done with the input data history update !\n\n\nGenerate the History for remapping in Use Cases 1-1,2,3\n\n\n\n\nin the workflow top menu of Galaxy, select the \nMetavisitor: Workflow for remapping in Use Cases 1-1,2,3\n workflow and directly select the \nrun\noption (you may also look at the workflow before by selection the \nedit\n option).\n\n\nSpecify \"SRP013822\" for the step 1 option\n\n\nSpecify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection)\n\n\nclick at the bottom the checkbox \nSend results to a new history\n\n\nEdit the field that shows up by typing in it: \"\nHistory for remapping in Use Cases 1-1,2,3\n\"\n\n\nexecute the workflow by clicking the \nRun workflow\n button.\n\n\nAfter few seconds, you may follow the link to the new history running !\n\n\n\n\nHistory for remapping in Use Case 1-4\n\n\nThis is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow).\n\n\nNavigate back again to your \"base\" history \nInput data for Use Cases 1-1, 1-2, 1-3 and 1-4\n\n\nNow the sequence of operations to be performed should be more familiar to you:\n\n\n\n\nTop menu \nWorkflow\n\n\nSelect \nMetavisitor: Workflow for Use Case 1-4\n and the \nrun\n option\n\n\nStep 1 (Input Dataset Collection), select the \nSRP013822\n option\n\n\nStep 2 (viral nucleotide BLAST database), select \nnucleotide vir1 blast database\n (forced option if everything went well - only one blast database is available in this input history)\n\n\nclick at the bottom the checkbox \nSend results to a new history\n\n\nEdit the field that shows up by typing in it: \"\nHistory for Use Case 1-4\n\"\n\n\nexecute the workflow by clicking the \nRun workflow\n button.\n\n\nAfter few seconds, you may follow the link to the new \"\nHistory for Use Case 1-4\n\" running !", 
            "title": "Use Cases 1-1 to 1-4"
        }, 
        {
            "location": "/use_case_1/#histories-for-use-cases-1-1-1-2-1-3-and-1-4", 
            "text": "As you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start  from  the  Input data for Use Cases 1-1, 1-2, 1-3 and 1-4  history, run the appropriate workflow,  sending the outputs of the workflow in a new history  named accordingly.", 
            "title": "Histories for Use Cases 1-1, 1-2, 1-3 and 1-4"
        }, 
        {
            "location": "/use_case_1/#history-for-use-case-1-1", 
            "text": "", 
            "title": "History for Use Case 1-1."
        }, 
        {
            "location": "/use_case_1/#1-as-aforementioned-ensure-that-you-are-in-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history", 
            "text": "You can always control this by using the top menu  Users  --   Saved History  and selecting the desired history. If you don't see the History right bar, just click in addition the top menu  Analyze Data", 
            "title": "1. As aforementioned, ensure that you are in the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history."
        }, 
        {
            "location": "/use_case_1/#2-select-the-appropriate-workflow", 
            "text": "Click now on the  Workflow  top menu  Select the workflow  \"Metavisitor: Workflow for Use Case 1-1 (imported from API)\"  and to see the workflow, select the submenu  \"Edit\"   Now that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting  \"Run\"", 
            "title": "2. Select the appropriate workflow"
        }, 
        {
            "location": "/use_case_1/#3-select-the-appropriate-parameters-before-running-the-workflow", 
            "text": "You now see a page with all the workflow steps, whose top part looks like:     As pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the  ncbi_guide_ID . In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the  ncbi_guide_ID  field the  NC_007919.3_ value. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets.    You have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the  previous chapter ), there is no other option in the menu than \" SRP013822 \".    You have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \" nucleotide vir1 blast database \", just because there is only one dataset in the input history with the \"blast database\" type.    You can review the other steps of the workflow. But there is no other selection to perform before running the workflow.", 
            "title": "3. Select the appropriate parameters before running the workflow"
        }, 
        {
            "location": "/use_case_1/#4-running-the-workflow-sending-the-outputs-in-a-new-history", 
            "text": "We are almost ready, but before clicking the \" Run Workflow \" button there is an important thing to do:\n- Check the \" Send results to a new history \" checkbox as shown Here    And edit the field to \"History for Use Case 1-1\"  You can now click the \"Run workflow\" button. \nThis trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history.   When the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see:   Note that 24 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets", 
            "title": "4. Running the workflow sending the outputs in a new history"
        }, 
        {
            "location": "/use_case_1/#histories-for-use-cases-1-2-and-1-3", 
            "text": "Histories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1.  Do exactly as described for Use Case 1-1 and\n- Remember to go back to the  Input data for Use Cases 1-1, 1-2, 1-3 and 1-4  history and be sure you are going to run the workflow from that history.\n-  Select the appropriate workflow  !\n- Remember to Check the \" Send results to a new history \" checkbox, rename the new history appropriately before pressing the \" Run workflow \" button", 
            "title": "Histories for Use Cases 1-2 and 1-3"
        }, 
        {
            "location": "/use_case_1/#history-for-remapping-in-use-cases-1-123", 
            "text": "Before running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our  Input data for Use Cases 1-1, 1-2, 1-3 and 1-4  history.", 
            "title": "History for remapping in Use Cases 1-1,2,3"
        }, 
        {
            "location": "/use_case_1/#update-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history", 
            "text": "This is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408).  Thus, go back to the  Input data for Use Cases 1-1, 1-2, 1-3 and 1-4  history and   Use the  Retrieve FASTA from NCBI  Metavisitor tool to retrieve the NC_007919.3 sequence.  Use the  Regex Find And Replace  tool on the  Retrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3'  dataset as an input, and put  gi\\|346421290\\|ref\\|NC_007919.3\\|_Nora_virus,_complete_genome  as Find Regex parameter and  NC_007919.3  as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity.  Use the  Retrieve FASTA from NCBI  Metavisitor tool to retrieve the JX220408 sequence.  Use the  Regex Find And Replace  tool on the  Retrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408'  dataset as an input, and put  gi\\|402295620\\|gb\\|JX220408.1\\|_Nora_virus_isolate_FR1,_complete_genome  as Find Regex parameter and  JX220408.1  as Replacement parameter. Rename the generated dataset JX220408.1 for clarity.  click on the top wheel history icon, select  Copy Datasets ; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\".  Repeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset.  and Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset.  you may have to refresh your  Input data for Use Cases 1-1, 1-2, 1-3 and 1-4  history to reveal the three copied datasets  The last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the  For all selected...  button, select  Build Dataset List , name this list \"Nora virus genomes\", and press the  Create list  button.   We are done with the input data history update !", 
            "title": "update the Input data for Use Cases 1-1, 1-2, 1-3 and 1-4 history"
        }, 
        {
            "location": "/use_case_1/#generate-the-history-for-remapping-in-use-cases-1-123", 
            "text": "in the workflow top menu of Galaxy, select the  Metavisitor: Workflow for remapping in Use Cases 1-1,2,3  workflow and directly select the  run option (you may also look at the workflow before by selection the  edit  option).  Specify \"SRP013822\" for the step 1 option  Specify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection)  click at the bottom the checkbox  Send results to a new history  Edit the field that shows up by typing in it: \" History for remapping in Use Cases 1-1,2,3 \"  execute the workflow by clicking the  Run workflow  button.  After few seconds, you may follow the link to the new history running !", 
            "title": "Generate the History for remapping in Use Cases 1-1,2,3"
        }, 
        {
            "location": "/use_case_1/#history-for-remapping-in-use-case-1-4", 
            "text": "This is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow).", 
            "title": "History for remapping in Use Case 1-4"
        }, 
        {
            "location": "/use_case_1/#navigate-back-again-to-your-base-history-input-data-for-use-cases-1-1-1-2-1-3-and-1-4", 
            "text": "Now the sequence of operations to be performed should be more familiar to you:   Top menu  Workflow  Select  Metavisitor: Workflow for Use Case 1-4  and the  run  option  Step 1 (Input Dataset Collection), select the  SRP013822  option  Step 2 (viral nucleotide BLAST database), select  nucleotide vir1 blast database  (forced option if everything went well - only one blast database is available in this input history)  click at the bottom the checkbox  Send results to a new history  Edit the field that shows up by typing in it: \" History for Use Case 1-4 \"  execute the workflow by clicking the  Run workflow  button.  After few seconds, you may follow the link to the new \" History for Use Case 1-4 \" running !", 
            "title": "Navigate back again to your \"base\" history Input data for Use Cases 1-1, 1-2, 1-3 and 1-4"
        }, 
        {
            "location": "/use_case_2/", 
            "text": "Histories for Use Cases 2-1, 2-2\n\n\nNow that you get more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4 described in details in the previous chapters, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step.\n\n\nInput data for Use Cases 2-1 and 2-2\n\n\nAs for the previous Use Case 1, the first step is to collect all input data in an history that we will name \nInput data for Use Cases 2-1 and 2-2\n\n\n\n\nCreate a new history\n\n\nRename this history \nInput data for Use Cases 2-1 and 2-2\n\n\nFor the small RNA sequence datasets (ERP012577) in this study, we are going to use another tool to upload to the Galaxy Metavisitor server: the \nEBI SRA ENA SRA\ntool which in the \"Get data\" section of the left tool bar.\n\n\nclick on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the \nERP012577\n link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar).\n\n\nrepeat the exact same operation, for the three other \"fastq file 1\".\n\n\nat final you should upload four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\"\n\n\nOnce the 4 uploads are \ncompleted\n (may takes minutes, depending on your network speed connection), click on the pencil icon of the 4 datasets, click on the \ndatatype\n tab and get it to \nfastqsanger\n.\n\n\n\n\n\n\nCreate a dataset collection as \npreviously explained\n and name it \nSmall RNA reads ERP012577\n\n\nFor the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again the \nEBI SRA ENA SRA\ntool which in the \"Get data\" section of the left tool bar.\n\n\nclick on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the \nERS977505\n link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar).\n\n\nrepeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\"\n\n\nat final you should upload four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\"\n\n\n\n\n\n\nCreate a dataset collection as explained in the previous chapter and name it \nlong read RNAseq datasets\n\n\nUsing the \"Upload file tool\" as explained \nbefore\n, upload the Plasmodium berghei genome by pasting this URL in the \nPaste/Fetch Data\n tab of the tools:\n\n\n\n\nftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz\n\n\n\n\n\n\nUse the \nRetrieve FASTA from NCBI\n, paste \nphix174[title]\n in the \"Query to NCBI in entrez format\" field and select \nnucleotide\n for the NCBI database. This will upload 174 fasta sequences from phix174.\n\n\nUse the wheel icon at the top of the history bar to copy \nnucleotide vir1 blast database\n and \nprotein vir1 blast database\n \nfrom\n the history \nReferences\n \nto\n the current history \nInput data for Use Cases 2-1 and 2-2\n. If you don't remember well how to copy datasets between histories, you may read again the explanation \nhere\n (step 4.)\n\n\n\n\nYour are now ready for generating Uses Cases 2-1 and 2-2\n\n\nHistory for Use Case 2-1\n\n\n\n\nStay in the current history \nInput data for Use Cases 2-1 and 2-2\n !\n\n\nIn the \nWorkflow\n menu, select the workflow \nMetavisitor: Workflow for Use Case 2-1\n and directly select \nRun\n (you may also look at the workflow using the \nedit\n option)\n\n\nBe careful at selecting \nSmall RNA reads ERP012577\n for the step 1 (Input Dataset Collection)\n\n\nFor the step 2, the option \nprotein vir1 blast database\n is forced, because the workflow is expecting of protein blast database for this step and only one dataset with this datatype is available in the history\n\n\nBe careful\n at selecting\n\n\n\n\nftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz\n\n\n\n\nfor \nstep 10\n (sRbowtie)\n6. Be careful at selecting\n\n\nRetrieve FASTA from NCBI (Nucleotide) with queryString 'phix174[title]'\n\n\n\n\nfor step 11 (sRbowtie).\n7. Click the \nSend results to a new history\n checkbox and rename the history to \"History for Use Case 2-1\".\n8. Run Workflow !\n\n\nYou may follow the link to the new history when the workflow is started.\n\n\nHistory for Use Case 2-2\n\n\n\n\nIf you are not already in, go back to the history \nInput data for Use Cases 2-1 and 2-2\n\n\nIn the \nWorkflow\n menu, select the workflow \nMetavisitor: Workflow for Use Case 2-2\n and directly select \nRun\n (you may also look at the workflow using the \nedit\n option)\n\n\nBe careful at selecting \nlong read RNAseq datasets\n for the step 1 (Input Dataset Collection)\n\n\nFor the step 2, the option \nprotein vir1 blast database\n is forced, because the workflow is expecting of protein blast database for this step and only one dataset with this datatype is available in the history\n\n\nClick the \nSend results to a new history\n checkbox and rename the history to \"History for Use Case 2-1\".\n\n\nRun Workflow !", 
            "title": "Use Cases 2-1 and 2-2"
        }, 
        {
            "location": "/use_case_2/#histories-for-use-cases-2-1-2-2", 
            "text": "Now that you get more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4 described in details in the previous chapters, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step.", 
            "title": "Histories for Use Cases 2-1, 2-2"
        }, 
        {
            "location": "/use_case_2/#input-data-for-use-cases-2-1-and-2-2", 
            "text": "As for the previous Use Case 1, the first step is to collect all input data in an history that we will name  Input data for Use Cases 2-1 and 2-2   Create a new history  Rename this history  Input data for Use Cases 2-1 and 2-2  For the small RNA sequence datasets (ERP012577) in this study, we are going to use another tool to upload to the Galaxy Metavisitor server: the  EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar.  click on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the  ERP012577  link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar).  repeat the exact same operation, for the three other \"fastq file 1\".  at final you should upload four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\"  Once the 4 uploads are  completed  (may takes minutes, depending on your network speed connection), click on the pencil icon of the 4 datasets, click on the  datatype  tab and get it to  fastqsanger .    Create a dataset collection as  previously explained  and name it  Small RNA reads ERP012577  For the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again the  EBI SRA ENA SRA tool which in the \"Get data\" section of the left tool bar.  click on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the  ERS977505  link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar).  repeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\"  at final you should upload four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\"    Create a dataset collection as explained in the previous chapter and name it  long read RNAseq datasets  Using the \"Upload file tool\" as explained  before , upload the Plasmodium berghei genome by pasting this URL in the  Paste/Fetch Data  tab of the tools:   ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz   Use the  Retrieve FASTA from NCBI , paste  phix174[title]  in the \"Query to NCBI in entrez format\" field and select  nucleotide  for the NCBI database. This will upload 174 fasta sequences from phix174.  Use the wheel icon at the top of the history bar to copy  nucleotide vir1 blast database  and  protein vir1 blast database   from  the history  References   to  the current history  Input data for Use Cases 2-1 and 2-2 . If you don't remember well how to copy datasets between histories, you may read again the explanation  here  (step 4.)   Your are now ready for generating Uses Cases 2-1 and 2-2", 
            "title": "Input data for Use Cases 2-1 and 2-2"
        }, 
        {
            "location": "/use_case_2/#history-for-use-case-2-1", 
            "text": "Stay in the current history  Input data for Use Cases 2-1 and 2-2  !  In the  Workflow  menu, select the workflow  Metavisitor: Workflow for Use Case 2-1  and directly select  Run  (you may also look at the workflow using the  edit  option)  Be careful at selecting  Small RNA reads ERP012577  for the step 1 (Input Dataset Collection)  For the step 2, the option  protein vir1 blast database  is forced, because the workflow is expecting of protein blast database for this step and only one dataset with this datatype is available in the history  Be careful  at selecting   ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz  for  step 10  (sRbowtie)\n6. Be careful at selecting  Retrieve FASTA from NCBI (Nucleotide) with queryString 'phix174[title]'  for step 11 (sRbowtie).\n7. Click the  Send results to a new history  checkbox and rename the history to \"History for Use Case 2-1\".\n8. Run Workflow !  You may follow the link to the new history when the workflow is started.", 
            "title": "History for Use Case 2-1"
        }, 
        {
            "location": "/use_case_2/#history-for-use-case-2-2", 
            "text": "If you are not already in, go back to the history  Input data for Use Cases 2-1 and 2-2  In the  Workflow  menu, select the workflow  Metavisitor: Workflow for Use Case 2-2  and directly select  Run  (you may also look at the workflow using the  edit  option)  Be careful at selecting  long read RNAseq datasets  for the step 1 (Input Dataset Collection)  For the step 2, the option  protein vir1 blast database  is forced, because the workflow is expecting of protein blast database for this step and only one dataset with this datatype is available in the history  Click the  Send results to a new history  checkbox and rename the history to \"History for Use Case 2-1\".  Run Workflow !", 
            "title": "History for Use Case 2-2"
        }, 
        {
            "location": "/use_case_3-1/", 
            "text": "Now that you get more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4 described in details in the previous chapters, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the \nprevious chapters\n step by step.\n\n\nInput data for Use Case 3-1\n\n\nAs for the previous Use Cases 1 and 2, the first step is to collect all input data in an history that we will name \nInput data for Use Case 3-1\n. \n\n\n\n\nCreate a new history\n\n\nRename this history \nInput data for Use Case 3-1\n\n\n\n\nWe are going to upload 40 datasets for the EBI ENA SRP068722. This is a bit tedious, but you follow the instructions bellow, it is not difficult.\n\n\n\n\nUse the tool \nExtract reads in FASTQ/A format from NCBI SRA\n, fill the SRR accession field with the first EBI SRA identifier \nSRR3111582\n and let the \noutput format\n selected as fastq. Click the \nExecute\n button.\n\n\nNow that the tool has started to run, you can click on the dataset in order to expand the information. Then click on the rerun icon (two curved arrows, when the mouse pass over the icon you can see the info \"Run this job again\" popping up). The only thing you have to do is to edit the SRR accession field from \nSRR3111582\n to \nSRR3111583\n, and press the  \nExecute\n button.\n\n\nJust repeat this operation with all the datasets we want to upload. Here is the full list of the SRR identifiers for the 40 datasets we will upload. Note that these identifiers increment by 1 from \nSRR3111582\n to \nSRR3111622\n with \none exception\n: we go directly from \nSRR3111614\n to \nSRR3111614\n.\n\n\n\n\nSRR3111582\nSRR3111583\nSRR3111584\nSRR3111585\nSRR3111586\nSRR3111587\nSRR3111588\nSRR3111589\nSRR3111590\nSRR3111591\nSRR3111592\nSRR3111593\nSRR3111594\nSRR3111595\nSRR3111596\nSRR3111597\nSRR3111598\nSRR3111599\nSRR3111600\nSRR3111601\nSRR3111602\nSRR3111603\nSRR3111604\nSRR3111605\nSRR3111606\nSRR3111607\nSRR3111608\nSRR3111609\nSRR3111610\nSRR3111611\nSRR3111612\nSRR3111613\nSRR3111614\nSRR3111616\nSRR3111617\nSRR3111618\nSRR3111619\nSRR3111620\nSRR3111621\nSRR3111622\n\n\n\n\n\n\nFor each of the 40 imported datasets, click the pencil icon, and change the datatype to \nfastqsanger\n. Be systematic, do it for all datasets 1 to 40.\n\n\n\n\nClick on the checked box icon in the history top menu, check all 40 datasets (\nAll\n button), and \nFor all selected\n, \nBuild a dataset list\n that you name \"SRP068722\".\n\n\nCopy the \nvir1 nucleotide BLAST database\n from the \nReferences\n history to the current history \nInput data for Use Case 3-1\n.\n\n\nNow we still have to associate sequencing dataset coming from a same patient. We are going to use the tool \nConcatenate multiple datasets\n to merge multiple datasets in a same fastq file.\n\n\nFor patient 0450-318, use \nConcatenate multiple datasets\n and select the datasets SRR3111582 to SRR3111587. Run the tool and rename the dataset \"patient 0450-318\"\n\n\nFor patient 0387-272, use \nConcatenate multiple datasets\n and select the datasets SRR3111588 to SRR3111593. Run the tool and rename the dataset \"patient 0387-272\"\n\n\nFor patient 0629-453, use \nConcatenate multiple datasets\n and select the datasets SRR3111594 to SRR3111599. Run the tool and rename the dataset \"patient 0629-453\"\n\n\nFor patient 0444-312, use \nConcatenate multiple datasets\n and select the datasets SRR3111600 to SRR3111603. Run the tool and rename the dataset \"patient 0444-312\"\n\n\nFor patient 0500-355neg, use \nConcatenate multiple datasets\n and select the datasets SRR3111604 and SRR3111605. Run the tool and rename the dataset \"patient 0500-355neg\"\n\n\nFor patient 0292-xxxneg, use \nConcatenate multiple datasets\n and select the datasets SRR3111606 and SRR3111607. Run the tool and rename the dataset \"patient 0292-xxxneg\"\n\n\nFor patient 0394-274, use \nConcatenate multiple datasets\n and select the datasets SRR3111608 and SRR3111609. Run the tool and rename the dataset \"patient 0394-274\"\n\n\nFor patient 0218-162neg, use \nConcatenate multiple datasets\n and select the datasets SRR3111610 and SRR3111611. Run the tool and rename the dataset \"patient 0218-162neg\"\n\n\nFor patient 0311-217HIVneg, use \nConcatenate multiple datasets\n and select the datasets SRR3111612 and SRR3111613. Run the tool and rename the dataset \"patient 0311-217HIVneg\"\n\n\nFor patient 0440-307neg, use \nConcatenate multiple datasets\n and select the datasets SRR3111614 and SRR3111616. Run the tool and rename the dataset \"patient 0440-307neg\"\n\n\nFor patient 0518-370neg, use \nConcatenate multiple datasets\n and select the datasets SRR3111617 and SRR3111618. Run the tool and rename the dataset \"patient 0518-370neg\"\n\n\nFor patient 0560-420neg, use \nConcatenate multiple datasets\n and select the datasets SRR3111619 and SRR3111620. Run the tool and rename the dataset \"patient 0560-420neg\"\n\n\nFor patient 0575-419neg, use \nConcatenate multiple datasets\n and select the datasets SRR3111621 and SRR3111622. Run the tool and rename the dataset \"patient 0575-419neg\"\n\n\n\n\n\n\nThe last action to perform in this history is to create a dataset collection of patient datasets: Click on the checked box icon in the history top menu, check the \"patient... \" datasets we have just generated by concatenation (13 datasets), and \nFor all selected\n, \nBuild a dataset list\n that you name \"patient collection\".\n\n\nWe are done.\n\n\n\n\nHistory for Use Case 3-1\n\n\n\n\nStay in the history \nInput data for Use Case 3-1\n\n\npick the workflow \nMetavisitor: Workflow for Use Case 3-1\n in the workflows menu, and select the \nrun\n option.\n\n\nFor Step 1 (Fever Patient Sequences collection), select \npatient collection\n (this should be already selected).\n\n\nFor Step 2, select the \nnucleotide vir1 blast database\n (this should also be already selected)\n\n\nAs usual, check the box \nSend results to a new history\n, edit the name of the new history to \nHistory for Use Case 3-1\n, and \nExecute\n the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up.", 
            "title": "Use Case 3-1"
        }, 
        {
            "location": "/use_case_3-1/#input-data-for-use-case-3-1", 
            "text": "As for the previous Use Cases 1 and 2, the first step is to collect all input data in an history that we will name  Input data for Use Case 3-1 .    Create a new history  Rename this history  Input data for Use Case 3-1   We are going to upload 40 datasets for the EBI ENA SRP068722. This is a bit tedious, but you follow the instructions bellow, it is not difficult.   Use the tool  Extract reads in FASTQ/A format from NCBI SRA , fill the SRR accession field with the first EBI SRA identifier  SRR3111582  and let the  output format  selected as fastq. Click the  Execute  button.  Now that the tool has started to run, you can click on the dataset in order to expand the information. Then click on the rerun icon (two curved arrows, when the mouse pass over the icon you can see the info \"Run this job again\" popping up). The only thing you have to do is to edit the SRR accession field from  SRR3111582  to  SRR3111583 , and press the   Execute  button.  Just repeat this operation with all the datasets we want to upload. Here is the full list of the SRR identifiers for the 40 datasets we will upload. Note that these identifiers increment by 1 from  SRR3111582  to  SRR3111622  with  one exception : we go directly from  SRR3111614  to  SRR3111614 .   SRR3111582\nSRR3111583\nSRR3111584\nSRR3111585\nSRR3111586\nSRR3111587\nSRR3111588\nSRR3111589\nSRR3111590\nSRR3111591\nSRR3111592\nSRR3111593\nSRR3111594\nSRR3111595\nSRR3111596\nSRR3111597\nSRR3111598\nSRR3111599\nSRR3111600\nSRR3111601\nSRR3111602\nSRR3111603\nSRR3111604\nSRR3111605\nSRR3111606\nSRR3111607\nSRR3111608\nSRR3111609\nSRR3111610\nSRR3111611\nSRR3111612\nSRR3111613\nSRR3111614\nSRR3111616\nSRR3111617\nSRR3111618\nSRR3111619\nSRR3111620\nSRR3111621\nSRR3111622    For each of the 40 imported datasets, click the pencil icon, and change the datatype to  fastqsanger . Be systematic, do it for all datasets 1 to 40.   Click on the checked box icon in the history top menu, check all 40 datasets ( All  button), and  For all selected ,  Build a dataset list  that you name \"SRP068722\".  Copy the  vir1 nucleotide BLAST database  from the  References  history to the current history  Input data for Use Case 3-1 .  Now we still have to associate sequencing dataset coming from a same patient. We are going to use the tool  Concatenate multiple datasets  to merge multiple datasets in a same fastq file.  For patient 0450-318, use  Concatenate multiple datasets  and select the datasets SRR3111582 to SRR3111587. Run the tool and rename the dataset \"patient 0450-318\"  For patient 0387-272, use  Concatenate multiple datasets  and select the datasets SRR3111588 to SRR3111593. Run the tool and rename the dataset \"patient 0387-272\"  For patient 0629-453, use  Concatenate multiple datasets  and select the datasets SRR3111594 to SRR3111599. Run the tool and rename the dataset \"patient 0629-453\"  For patient 0444-312, use  Concatenate multiple datasets  and select the datasets SRR3111600 to SRR3111603. Run the tool and rename the dataset \"patient 0444-312\"  For patient 0500-355neg, use  Concatenate multiple datasets  and select the datasets SRR3111604 and SRR3111605. Run the tool and rename the dataset \"patient 0500-355neg\"  For patient 0292-xxxneg, use  Concatenate multiple datasets  and select the datasets SRR3111606 and SRR3111607. Run the tool and rename the dataset \"patient 0292-xxxneg\"  For patient 0394-274, use  Concatenate multiple datasets  and select the datasets SRR3111608 and SRR3111609. Run the tool and rename the dataset \"patient 0394-274\"  For patient 0218-162neg, use  Concatenate multiple datasets  and select the datasets SRR3111610 and SRR3111611. Run the tool and rename the dataset \"patient 0218-162neg\"  For patient 0311-217HIVneg, use  Concatenate multiple datasets  and select the datasets SRR3111612 and SRR3111613. Run the tool and rename the dataset \"patient 0311-217HIVneg\"  For patient 0440-307neg, use  Concatenate multiple datasets  and select the datasets SRR3111614 and SRR3111616. Run the tool and rename the dataset \"patient 0440-307neg\"  For patient 0518-370neg, use  Concatenate multiple datasets  and select the datasets SRR3111617 and SRR3111618. Run the tool and rename the dataset \"patient 0518-370neg\"  For patient 0560-420neg, use  Concatenate multiple datasets  and select the datasets SRR3111619 and SRR3111620. Run the tool and rename the dataset \"patient 0560-420neg\"  For patient 0575-419neg, use  Concatenate multiple datasets  and select the datasets SRR3111621 and SRR3111622. Run the tool and rename the dataset \"patient 0575-419neg\"    The last action to perform in this history is to create a dataset collection of patient datasets: Click on the checked box icon in the history top menu, check the \"patient... \" datasets we have just generated by concatenation (13 datasets), and  For all selected ,  Build a dataset list  that you name \"patient collection\".  We are done.", 
            "title": "Input data for Use Case 3-1"
        }, 
        {
            "location": "/use_case_3-1/#history-for-use-case-3-1", 
            "text": "Stay in the history  Input data for Use Case 3-1  pick the workflow  Metavisitor: Workflow for Use Case 3-1  in the workflows menu, and select the  run  option.  For Step 1 (Fever Patient Sequences collection), select  patient collection  (this should be already selected).  For Step 2, select the  nucleotide vir1 blast database  (this should also be already selected)  As usual, check the box  Send results to a new history , edit the name of the new history to  History for Use Case 3-1 , and  Execute  the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up.", 
            "title": "History for Use Case 3-1"
        }, 
        {
            "location": "/use_case_3-2/", 
            "text": "Input data for Use Case 3-2\n\n\nAs for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name \nInput data for Use Case 3-2\n. \n\n\n\n\nCreate a new history\n\n\nRename this history \nInput data for Use Case 3-1\n\n\nUsing the tool \nExtract reads in FASTQ/A format from NCBI SRA\n, we are going to upload 42 paired end datasets. Indeed, these 42 datasets correspond to 84 fastq paired-ended sequence files. However, the \nExtract reads in FASTQ/A format from NCBI SRA\n directly merges two paired-end fastq datasets in a single file. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool \nConcatenate multiple datasets tail-to-head\n and delete and purge the original datasets. In all cases, we will rename the dataset with the patient id as indicated bellow, and change the datatype from fastq to fastqsanger.\nHere is a table that recapitulates the actions to perform, line by line.\n\n\n\n\n#ENA-RUN            action                                              post-action\nSRR453487           |rename \npatient 566\n                               |change datatype to \nfastq\n\nSRR453437           |rename \npatient 438\n                               |change datatype to \nfastq\n\nSRR453443,SRR453458 |concatenate, rename merge dataset  \npatient 401\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453430           |rename \npatient 382\n                               |change datatype to \nfastq\n\nSRR453491           |rename \npatient 377\n                               |change datatype to \nfastq\n\nSRR453499           |rename \npatient 375\n                               |change datatype to \nfastq\n\nSRR453484           |rename \npatient 350\n                               |change datatype to \nfastq\n\nSRR453464           |rename \npatient 349\n                               |change datatype to \nfastq\n\nSRR453506           |rename \npatient 345\n                               |change datatype to \nfastq\n\nSRR453417           |rename \npatient 344\n                               |change datatype to \nfastq\n\nSRR453490           |rename \npatient 335\n                               |change datatype to \nfastq\n\nSRR453478           |rename \npatient 331\n                               |change datatype to \nfastq\n\nSRR453465,SRR453480 |concatenate, rename merge dataset  \npatient 330\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453489,SRR453505 |concatenate, rename merge dataset  \npatient 329\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453498           |rename \npatient 322\n                               |change datatype to \nfastq\n\nSRR453446           |rename \npatient 321\n                               |change datatype to \nfastq\n\nSRR453427,SRR453440 |concatenate, rename merge dataset  \npatient 315\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453438           |rename \npatient 282\n                               |change datatype to \nfastq\n\nSRR453450           |rename \npatient 275\n                               |change datatype to \nfastq\n\nSRR453460           |rename \npatient 274\n                               |change datatype to \nfastq\n\nSRR453485           |rename \npatient 270\n                               |change datatype to \nfastq\n\nSRR453448           |rename \npatient 266\n                               |change datatype to \nfastq\n\nSRR453424,SRR453457 |concatenate, rename merge dataset  \npatient 263\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453510           |rename \npatient 193\n                               |change datatype to \nfastq\n\nSRR453456           |rename \npatient 187\n                               |change datatype to \nfastq\n\nSRR453425,SRR453469 |concatenate, rename merge dataset  \npatient 186\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453481           |rename \npatient 183\n                               |change datatype to \nfastq\n\nSRR453531           |rename \npatient 180\n                               |change datatype to \nfastq\n\nSRR453474           |rename \npatient 179\n                               |change datatype to \nfastq\n\nSRR453509           |rename \npatient 171\n                               |change datatype to \nfastq\n\nSRR453451           |rename \npatient 168\n                               |change datatype to \nfastq\n\nSRR453495,SRR453504 |concatenate, rename merge dataset  \npatient 161\n   |change the merged dataset to datatype to \nfastq\n, and delete and purge the original SRR datasets\nSRR453500           |rename \npatient 159\n                               |change datatype to \nfastq\n\nSRR453493           |rename \npatient 156\n                               |change datatype to \nfastq\n\nSRR453444           |rename \npatient 131\n                               |change datatype to \nfastq\n\nSRR453426           |rename \npatient 78                                 |change datatype to \nfastq\n\n\n\n\n\n\n\nCreate a dataset collection of patient datasets: Click on the checked box icon in the history top menu, check the \"patient... \" datasets we have just generated (36 datasets) (you can use the \nSelect all\n button), and \nFor all selected\n, \nBuild a dataset list\n that you name \"Tractable Patient Datasets\".\n\n\nCopy the \nvir1 nucleotide BLAST database\n from the \nReferences\n history to the current history \nInput data for Use Case 3-2\n.\n\n\n\n\nHistory for Use Case 3-2\n\n\n\n\nStay in the history \nInput data for Use Case 3-2\n\n\npick the workflow \nMetavisitor: Workflow for Use Case 3-2\n in the workflows menu, and select the \nrun\n option.\n\n\nFor Step 1 (Fever Patient Sequences collection), select \nTractable Patient Datasets\n (this should be already selected).\n\n\nFor Step 2, select the \nnucleotide vir1 blast database\n (this should also be already selected)\n\n\nAs usual, check the box \nSend results to a new history\n, edit the name of the new history to \nHistory for Use Case 3-2\n, and \nExecute\n the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your \nUser\n -\n \nSaved history\n menu, you will see you \nHistory for Use Case 3-2\n running and you will be able to access it.\n\n\n\n\nAs a last note, the workflow for Use Case 3-2 may take a long time. Be patient.", 
            "title": "Use Case 3-2"
        }, 
        {
            "location": "/use_case_3-2/#input-data-for-use-case-3-2", 
            "text": "As for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name  Input data for Use Case 3-2 .    Create a new history  Rename this history  Input data for Use Case 3-1  Using the tool  Extract reads in FASTQ/A format from NCBI SRA , we are going to upload 42 paired end datasets. Indeed, these 42 datasets correspond to 84 fastq paired-ended sequence files. However, the  Extract reads in FASTQ/A format from NCBI SRA  directly merges two paired-end fastq datasets in a single file. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool  Concatenate multiple datasets tail-to-head  and delete and purge the original datasets. In all cases, we will rename the dataset with the patient id as indicated bellow, and change the datatype from fastq to fastqsanger.\nHere is a table that recapitulates the actions to perform, line by line.   #ENA-RUN            action                                              post-action\nSRR453487           |rename  patient 566                                |change datatype to  fastq \nSRR453437           |rename  patient 438                                |change datatype to  fastq \nSRR453443,SRR453458 |concatenate, rename merge dataset   patient 401    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453430           |rename  patient 382                                |change datatype to  fastq \nSRR453491           |rename  patient 377                                |change datatype to  fastq \nSRR453499           |rename  patient 375                                |change datatype to  fastq \nSRR453484           |rename  patient 350                                |change datatype to  fastq \nSRR453464           |rename  patient 349                                |change datatype to  fastq \nSRR453506           |rename  patient 345                                |change datatype to  fastq \nSRR453417           |rename  patient 344                                |change datatype to  fastq \nSRR453490           |rename  patient 335                                |change datatype to  fastq \nSRR453478           |rename  patient 331                                |change datatype to  fastq \nSRR453465,SRR453480 |concatenate, rename merge dataset   patient 330    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453489,SRR453505 |concatenate, rename merge dataset   patient 329    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453498           |rename  patient 322                                |change datatype to  fastq \nSRR453446           |rename  patient 321                                |change datatype to  fastq \nSRR453427,SRR453440 |concatenate, rename merge dataset   patient 315    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453438           |rename  patient 282                                |change datatype to  fastq \nSRR453450           |rename  patient 275                                |change datatype to  fastq \nSRR453460           |rename  patient 274                                |change datatype to  fastq \nSRR453485           |rename  patient 270                                |change datatype to  fastq \nSRR453448           |rename  patient 266                                |change datatype to  fastq \nSRR453424,SRR453457 |concatenate, rename merge dataset   patient 263    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453510           |rename  patient 193                                |change datatype to  fastq \nSRR453456           |rename  patient 187                                |change datatype to  fastq \nSRR453425,SRR453469 |concatenate, rename merge dataset   patient 186    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453481           |rename  patient 183                                |change datatype to  fastq \nSRR453531           |rename  patient 180                                |change datatype to  fastq \nSRR453474           |rename  patient 179                                |change datatype to  fastq \nSRR453509           |rename  patient 171                                |change datatype to  fastq \nSRR453451           |rename  patient 168                                |change datatype to  fastq \nSRR453495,SRR453504 |concatenate, rename merge dataset   patient 161    |change the merged dataset to datatype to  fastq , and delete and purge the original SRR datasets\nSRR453500           |rename  patient 159                                |change datatype to  fastq \nSRR453493           |rename  patient 156                                |change datatype to  fastq \nSRR453444           |rename  patient 131                                |change datatype to  fastq \nSRR453426           |rename  patient 78                                 |change datatype to  fastq    Create a dataset collection of patient datasets: Click on the checked box icon in the history top menu, check the \"patient... \" datasets we have just generated (36 datasets) (you can use the  Select all  button), and  For all selected ,  Build a dataset list  that you name \"Tractable Patient Datasets\".  Copy the  vir1 nucleotide BLAST database  from the  References  history to the current history  Input data for Use Case 3-2 .", 
            "title": "Input data for Use Case 3-2"
        }, 
        {
            "location": "/use_case_3-2/#history-for-use-case-3-2", 
            "text": "Stay in the history  Input data for Use Case 3-2  pick the workflow  Metavisitor: Workflow for Use Case 3-2  in the workflows menu, and select the  run  option.  For Step 1 (Fever Patient Sequences collection), select  Tractable Patient Datasets  (this should be already selected).  For Step 2, select the  nucleotide vir1 blast database  (this should also be already selected)  As usual, check the box  Send results to a new history , edit the name of the new history to  History for Use Case 3-2 , and  Execute  the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your  User  -   Saved history  menu, you will see you  History for Use Case 3-2  running and you will be able to access it.   As a last note, the workflow for Use Case 3-2 may take a long time. Be patient.", 
            "title": "History for Use Case 3-2"
        }, 
        {
            "location": "/development/", 
            "text": "Fixing submodule issues\n\n\nThis project is using roles from different places, and these are included as git submodules. \nIn a development cycle it may occur that the url of a repo changes, or that we have removed certain commits (We aim to avoid this). \nIf at any point you find yourself in the situation where a git checkout of submodules does not work, you can follow this procedure: \n(galaxyprojectdotorg.galaxy-extras as an example, replace this with the problematic role)\n\n\nSkip the git clone if you already cloned the repo.\n\n\ngit clone --recursive https://github.com/ARTbio/ansible-artimed.git\ncd ansible-artimed/\n\n\n\n\nEnter the directory of the problematic submodule\n\n\ncd roles/galaxyprojectdotorg.galaxy-extras/\n\n\n\n\nVerify that you have the correct remote.\nYou can find the correct remote in ansible-artimed/.gitmodules\n\n\ngit remote -v\n\n\n\n\nIf this is not the correct remote, change back to the ansible-artimed directory and do\n\n\ngit submodule sync\n\n\n\n\nGet the latest changesets (change back to the submodule directory if you had to sync the submodules)\n\n\ngit fetch\n\n\n\n\nCheck which branch you are on. You want to checkout the master branch.\n\n\n\n\n\ngit checkout master\ngit add galaxyprojectdotorg.galaxy-extras\ngit commit -m \"Fix galaxy-extras submodule revision\"\n```\n\n\nIf you push this work back to github and you compare your repo with ansible-artimed/master, you should not see any changes\nthat look like \nSubproject commit 8265bdceaf0eca5ea4daeda06eb0d28583754ef6\n (unless you have intentionally updated a submodule).", 
            "title": "Development"
        }, 
        {
            "location": "/development/#fixing-submodule-issues", 
            "text": "This project is using roles from different places, and these are included as git submodules. \nIn a development cycle it may occur that the url of a repo changes, or that we have removed certain commits (We aim to avoid this). \nIf at any point you find yourself in the situation where a git checkout of submodules does not work, you can follow this procedure: \n(galaxyprojectdotorg.galaxy-extras as an example, replace this with the problematic role)  Skip the git clone if you already cloned the repo.  git clone --recursive https://github.com/ARTbio/ansible-artimed.git\ncd ansible-artimed/  Enter the directory of the problematic submodule  cd roles/galaxyprojectdotorg.galaxy-extras/  Verify that you have the correct remote.\nYou can find the correct remote in ansible-artimed/.gitmodules  git remote -v  If this is not the correct remote, change back to the ansible-artimed directory and do  git submodule sync  Get the latest changesets (change back to the submodule directory if you had to sync the submodules)  git fetch  Check which branch you are on. You want to checkout the master branch.  git checkout master\ngit add galaxyprojectdotorg.galaxy-extras\ngit commit -m \"Fix galaxy-extras submodule revision\"\n```  If you push this work back to github and you compare your repo with ansible-artimed/master, you should not see any changes\nthat look like  Subproject commit 8265bdceaf0eca5ea4daeda06eb0d28583754ef6  (unless you have intentionally updated a submodule).", 
            "title": "Fixing submodule issues"
        }
    ]
}