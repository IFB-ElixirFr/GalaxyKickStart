{
    "docs": [
        {
            "location": "/", 
            "text": "GalaxyKickstarter\n\n\nGalaxyKickstarter is an \nAnsible\n playbook designed for installing, testing, deploying and \nmaintaining production-grade Galaxy instances.\n\nIn the basic configuration, this includes:\n\n\n\n\npostgresql server as database backend \n\n\nnginx proxy \n\n\nslurm cluster\n\n\n\n\nIn adition, tools and workflows can be managed.", 
            "title": "Home"
        }, 
        {
            "location": "/#galaxykickstarter", 
            "text": "GalaxyKickstarter is an  Ansible  playbook designed for installing, testing, deploying and \nmaintaining production-grade Galaxy instances. \nIn the basic configuration, this includes:   postgresql server as database backend   nginx proxy   slurm cluster   In adition, tools and workflows can be managed.", 
            "title": "GalaxyKickstarter"
        }, 
        {
            "location": "/about/", 
            "text": "GalaxyKickstarter\n\n\nGalaxyKickstarter is an Ansible playbook designed to help you get one or more production-ready\n \nGalaxy servers\n based on Ubuntu within minutes, and to maintain these servers.\n\n\nOptionally, instances can be pre-loaded with tools and workflows.\n\n\nThe playbook has been tested on \n\n\n\n\nCloud Machines\n\n\nVagrant Boxes\n\n\nPhysical Servers \n\n\nDocker.\n\n\n\n\nGalaxyKickstarter has been developed at the \nARTbio platform\n and contains roles developed\nby the \nGalaxy team\n.\n\n\nList of roles included in this playbook\n\n\nansible-postgresql-objects\n\n\ngalaxy-extras role\n\n\ngalaxy-tools role\n\n\ngalaxy-os role\n\n\ngalaxy role", 
            "title": "What is GalaxyKickstarter"
        }, 
        {
            "location": "/about/#galaxykickstarter", 
            "text": "GalaxyKickstarter is an Ansible playbook designed to help you get one or more production-ready\n  Galaxy servers  based on Ubuntu within minutes, and to maintain these servers.  Optionally, instances can be pre-loaded with tools and workflows.  The playbook has been tested on    Cloud Machines  Vagrant Boxes  Physical Servers   Docker.   GalaxyKickstarter has been developed at the  ARTbio platform  and contains roles developed\nby the  Galaxy team .", 
            "title": "GalaxyKickstarter"
        }, 
        {
            "location": "/about/#list-of-roles-included-in-this-playbook", 
            "text": "ansible-postgresql-objects  galaxy-extras role  galaxy-tools role  galaxy-os role  galaxy role", 
            "title": "List of roles included in this playbook"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Getting Started\n\n\nGetting the playbook\n\n\nGalaxyKickstarter is hosted on \ngithub\n and makes use of submodules, so care\nneeds to be taken to also download the submodules. Cloning the repository for the first time can be done like this\n(note the \n--recursive\n):\n\n\ngit clone --recursive https://github.com/ARTbio/ansible-artimed.git\n\n\n\n\nThe playbook (here \ngalaxy.yml\n) should be in the ansible-artimed folder.\n\n\nls ansible-artimed/\nCONTRIBUTORS.md  docs  extra-files  galaxy.yml  group_vars  hosts\nLICENSE.txt  mkdocs.yml  pre-commit.sh  README.md  roles  Vagrantfile\n\n\n\n\nDeploying galaxy-kickstart on remote machines.\n\n\n\n\nInside the repository you will find a hosts file.\nThis is an example inventory.\n\n\n[artimed]\nlocalhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n...\n\n\n\n\nHere \n[artimed]\n is a group, that contains a machine called localhost.\nThe variables defined in \ngroup_vars/artimed\n will be applied to this host.\nAnsible will connect by ssh to this machine, using the ssh key in \n~/.ssh/id_rsa\n.\n\n\nIf you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine),\ncreate a new inventory, and change \nlocalhost\n to the IP address of that machine.\n\nansible_ssh_user=\nuser\n controls under which username to connect to this machine.\nThis user needs to have sudo rights.\n\n\nThen, run the plabook by typing:\n\n\nansible-playbook --inventory-file=\nyour_inventory\n galaxy.yml\n\n\n\n\nYou can put multiple machines in your inventory.\nIf you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped.\nWhenever you change a variable (see \ncustomizations\n), you need to run the playbook again.", 
            "title": "Getting started"
        }, 
        {
            "location": "/getting_started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#getting-the-playbook", 
            "text": "GalaxyKickstarter is hosted on  github  and makes use of submodules, so care\nneeds to be taken to also download the submodules. Cloning the repository for the first time can be done like this\n(note the  --recursive ):  git clone --recursive https://github.com/ARTbio/ansible-artimed.git  The playbook (here  galaxy.yml ) should be in the ansible-artimed folder.  ls ansible-artimed/\nCONTRIBUTORS.md  docs  extra-files  galaxy.yml  group_vars  hosts\nLICENSE.txt  mkdocs.yml  pre-commit.sh  README.md  roles  Vagrantfile", 
            "title": "Getting the playbook"
        }, 
        {
            "location": "/getting_started/#deploying-galaxy-kickstart-on-remote-machines", 
            "text": "Inside the repository you will find a hosts file.\nThis is an example inventory.  [artimed]\nlocalhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n...  Here  [artimed]  is a group, that contains a machine called localhost.\nThe variables defined in  group_vars/artimed  will be applied to this host.\nAnsible will connect by ssh to this machine, using the ssh key in  ~/.ssh/id_rsa .  If you would like to run this playbook on a remote machine by ssh (currently needs to be a debian-type machine),\ncreate a new inventory, and change  localhost  to the IP address of that machine. ansible_ssh_user= user  controls under which username to connect to this machine.\nThis user needs to have sudo rights.  Then, run the plabook by typing:  ansible-playbook --inventory-file= your_inventory  galaxy.yml  You can put multiple machines in your inventory.\nIf you run the playbook a second time, the process will be much faster, since steps that have already been executed will be skipped.\nWhenever you change a variable (see  customizations ), you need to run the playbook again.", 
            "title": "Deploying galaxy-kickstart on remote machines."
        }, 
        {
            "location": "/customizations/", 
            "text": "Customising the playbook\n\n\nWe strongly encourage users to read the \nansible inventory\n documentation first.\n\n\nMost settings should be editable without modifying the playbook directly,\ninstead variables can be set in group_vars and host vars.\n\n\nThe playbook comes with an example inventory file \nhosts\n.\n\n\n[artimed]\nlocalhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n[travis_bioblend]\nlocalhost ansible_connection=local\n[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].\n\n\n\n\n[artimed]\n, \n[travis_bioblend]\n and \n[aws]\n are predefined groups. Any host (here we only have localhost) that\nis added to one or multiple groups will have the corresponding group variables applied.\nGroup variables are defined in \ngroup_vars/[name of the group]\n and default variables are found in \n\n\ngroup_vars/all\n.\nAll variables defined in \ngroup_vars/all\n are overwritten in \ngroup_vars/[name of the group]\n.  \n\n\nFor instance the variable \nproftpd_nat_masquerade\n is set to \nfalse\n in \ngroup_vars/all\n, while hosts in the \n[aws]\n group\napply the \n[aws]\n group variables which set \nproftpd_nat_masquerade\n to true, so that hosts in the aws group will have\nthis aws-specific setting applied. Any combination of groups may be used.\n\n\nIf you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that\nthe host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains\nthe group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost,\nyour inventory file should look like this:\n\n\n[metavisitor]\nlocalhost\n\n\n\n\nYou can then run the playbook as usual:\n\n\nansible-playbook --inventory-file=\nyour_inventory_file\n galaxy.yml\n\n\n\n\nImportant variables\n\n\nWe aimed for this playbook to be reusable. We therefore made most variables configurable.\nThe group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file\nor you can use ansible group variables to selectively set the variables for certain hosts/groups. See the \nansible documentation\nabout group variables\n for details.\n\n\nThese most important variables are:\n\n\n\n\n\n\nansible_ssh_user - The login name used to access the target.\n\n\n\n\n\n\nansible_ssh_private_key_file - The ssh private key used to access the target.\n\n\n\n\n\n\ninstall_galaxy - True for install a Galaxy instance.\n\n\n\n\n\n\ninstall_tools - True for install the NGS tools.\n\n\n\n\n\n\nrun_data_manager - True for run the data manager procedure.\n\n\n\n\n\n\ngalaxy_user_name - The Operating System user name for galaxy process.\n\n\n\n\n\n\ngalaxy_server_dir - The home of Operating System user for galaxy process.\n\n\n\n\n\n\ngalaxy_admin - The admin galaxy user.\n\n\n\n\n\n\ngalaxy_admin_pw - The admin galaxy password.\n\n\n\n\n\n\ndefault_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production.\n\n\n\n\n\n\ngalaxy_tool_list - The files that constants the list of tools to be installed.\n\n\n\n\n\n\ngalaxy_data_managers - The reference genomes and indexes to be load and build.\n\n\n\n\n\n\ngalaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered.\n\n\n\n\n\n\ngalaxy_database - The persistent directory where postgresql will be installed or will be recovered.\n\n\n\n\n\n\ngalaxy_db - Connection string for galaxy-postgresql.\n\n\n\n\n\n\ngalaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).", 
            "title": "Customizations"
        }, 
        {
            "location": "/customizations/#customising-the-playbook", 
            "text": "We strongly encourage users to read the  ansible inventory  documentation first.  Most settings should be editable without modifying the playbook directly,\ninstead variables can be set in group_vars and host vars.  The playbook comes with an example inventory file  hosts .  [artimed]\nlocalhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n[travis_bioblend]\nlocalhost ansible_connection=local\n[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].  [artimed] ,  [travis_bioblend]  and  [aws]  are predefined groups. Any host (here we only have localhost) that\nis added to one or multiple groups will have the corresponding group variables applied.\nGroup variables are defined in  group_vars/[name of the group]  and default variables are found in   group_vars/all .\nAll variables defined in  group_vars/all  are overwritten in  group_vars/[name of the group] .    For instance the variable  proftpd_nat_masquerade  is set to  false  in  group_vars/all , while hosts in the  [aws]  group\napply the  [aws]  group variables which set  proftpd_nat_masquerade  to true, so that hosts in the aws group will have\nthis aws-specific setting applied. Any combination of groups may be used.  If you want to apply any of the changes you made to the variables you need to run the playbook again, making sure that\nthe host you are targeting is in the right group. The simplest way to do so is to use an inventory file that only contains\nthe group and the host you wish to target. If this is for example the group metavisitor, and you target the host localhost,\nyour inventory file should look like this:  [metavisitor]\nlocalhost  You can then run the playbook as usual:  ansible-playbook --inventory-file= your_inventory_file  galaxy.yml", 
            "title": "Customising the playbook"
        }, 
        {
            "location": "/customizations/#important-variables", 
            "text": "We aimed for this playbook to be reusable. We therefore made most variables configurable.\nThe group_vars/all file contains the variables we have chosen as defaults. You may override them either in this file\nor you can use ansible group variables to selectively set the variables for certain hosts/groups. See the  ansible documentation\nabout group variables  for details.  These most important variables are:    ansible_ssh_user - The login name used to access the target.    ansible_ssh_private_key_file - The ssh private key used to access the target.    install_galaxy - True for install a Galaxy instance.    install_tools - True for install the NGS tools.    run_data_manager - True for run the data manager procedure.    galaxy_user_name - The Operating System user name for galaxy process.    galaxy_server_dir - The home of Operating System user for galaxy process.    galaxy_admin - The admin galaxy user.    galaxy_admin_pw - The admin galaxy password.    default_admin_api_key - The api key for tool installation and download reference genomes throught galaxy data managers. To be removed in production.    galaxy_tool_list - The files that constants the list of tools to be installed.    galaxy_data_managers - The reference genomes and indexes to be load and build.    galaxy_data - The persistent directory where the galaxy config and database directories will be installed or will be recovered.    galaxy_database - The persistent directory where postgresql will be installed or will be recovered.    galaxy_db - Connection string for galaxy-postgresql.    galaxy_changeset_id - The release of Galaxy to be installed (master, dev or release_xx_xx).", 
            "title": "Important variables"
        }, 
        {
            "location": "/installing tools and workflows/", 
            "text": "Installing tools\n\n\n\n\nThis playbook includes the \nansible-galaxy-tools\n role which can be used\nto install tools and workflows into galaxy instances using the \nbioblend\n api.  \n\n\nCreating a tool_list.yml file\n\n\nTo install tools, you will need to prepare a list of tools in yaml format.\nA an example of a a tool list can be found in \nhere\n\n\ntools:\n- name: blast_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: blastx_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: bowtie2\n  owner: devteam\n  revisions:\n  - 019c2a81547a\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n\n\n\n\nwhen the revision is empty, the latest available revision will be installed.\n\ntool_panel_section_label will determine the tool panel section where the tools will be found.\n\n\nObtaining a tool_list.yml file\n\n\nWe can also obtain a tool list from a runnning galaxy instance.\nNote that for server running a galaxy release \n16.04, you need a galaxy API keys and bioblend.\nA script is included in the extra-files directory.\n\n\npython get_tool_yml_from_gi.py --galaxy \nmy_galaxy_url\n --api-key \nmy_admin_api_key\n --output-file \nmy_tool_list.yml\n\n\n\n\n\nAdding a tool_list.yml file to a group_variable files\n\n\nGroup variable files are in the group_vars directory.\n\n\nIf you would like to install tools, you need to reference the tool_list.yml in the group variable file.\nWe typically place additional files in the \nextra-files/\nhostname\n/\nhostname\n_tool_list.yml\n file.\n\n\nIf you would like to add tools to a group that is called metavisitor edit \ngroup_vars/metavisitor\n and add these lines:\n\n\ninstall_tools: true\ngalaxy_tools_tool_list: \nextra-files/metavisitor/metavisitor_tool_list.yml\n\n\n\n\n\nInstalling workflows\n\n\nYou can also make sure that workflows are available after running the playbook.\nAs with tools, place the workflows in \nextra-files/\nhostname\n/\nhostname\nworkflow_name\n.ga\n\nAdd these lines to the corresponding group_var file:\n\n\ngalaxy_tools_install_workflows: true\ngalaxy_tools_workflows:\n  - \nextra-files/metavisitor/Galaxy-Workflow-create_model.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga\n\n  - \nextra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga\n\n\n\n\n\nRunning the playbook\n\n\nAs per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group.\nIf the target is localhost, your inventory file should look ike this:\n\n\n[metavisitor]\nlocalhost\n\n\n\n\nthen run the playbook like so:\n\n\nansible-playbook --inventory-file=\nyour_inventory_file\n galaxy.yml", 
            "title": "Installing tools and workflows"
        }, 
        {
            "location": "/installing tools and workflows/#installing-tools", 
            "text": "This playbook includes the  ansible-galaxy-tools  role which can be used\nto install tools and workflows into galaxy instances using the  bioblend  api.", 
            "title": "Installing tools"
        }, 
        {
            "location": "/installing tools and workflows/#creating-a-tool_listyml-file", 
            "text": "To install tools, you will need to prepare a list of tools in yaml format.\nA an example of a a tool list can be found in  here  tools:\n- name: blast_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: blastx_to_scaffold\n  owner: drosofff\n  revisions:\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: bowtie2\n  owner: devteam\n  revisions:\n  - 019c2a81547a\n  tool_panel_section_label: Metavisitor\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/  when the revision is empty, the latest available revision will be installed. \ntool_panel_section_label will determine the tool panel section where the tools will be found.", 
            "title": "Creating a tool_list.yml file"
        }, 
        {
            "location": "/installing tools and workflows/#obtaining-a-tool_listyml-file", 
            "text": "We can also obtain a tool list from a runnning galaxy instance.\nNote that for server running a galaxy release  16.04, you need a galaxy API keys and bioblend.\nA script is included in the extra-files directory.  python get_tool_yml_from_gi.py --galaxy  my_galaxy_url  --api-key  my_admin_api_key  --output-file  my_tool_list.yml", 
            "title": "Obtaining a tool_list.yml file"
        }, 
        {
            "location": "/installing tools and workflows/#adding-a-tool_listyml-file-to-a-group_variable-files", 
            "text": "Group variable files are in the group_vars directory.  If you would like to install tools, you need to reference the tool_list.yml in the group variable file.\nWe typically place additional files in the  extra-files/ hostname / hostname _tool_list.yml  file.  If you would like to add tools to a group that is called metavisitor edit  group_vars/metavisitor  and add these lines:  install_tools: true\ngalaxy_tools_tool_list:  extra-files/metavisitor/metavisitor_tool_list.yml", 
            "title": "Adding a tool_list.yml file to a group_variable files"
        }, 
        {
            "location": "/installing tools and workflows/#installing-workflows", 
            "text": "You can also make sure that workflows are available after running the playbook.\nAs with tools, place the workflows in  extra-files/ hostname / hostname workflow_name .ga \nAdd these lines to the corresponding group_var file:  galaxy_tools_install_workflows: true\ngalaxy_tools_workflows:\n  -  extra-files/metavisitor/Galaxy-Workflow-create_model.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-separate_host_and_virus_reads.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-standart_metavisitor_workflow_(input__clipped_dataset).ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-1_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-2_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Metavisitor_Test_case_1-3_Guided.ga \n  -  extra-files/metavisitor/Galaxy-Workflow-Meta-visitor__test_case_Nora_virus,_REMAPPING.ga", 
            "title": "Installing workflows"
        }, 
        {
            "location": "/installing tools and workflows/#running-the-playbook", 
            "text": "As per usual, run the playbook with an inventory file that maps your target machine to the metavisitor group.\nIf the target is localhost, your inventory file should look ike this:  [metavisitor]\nlocalhost  then run the playbook like so:  ansible-playbook --inventory-file= your_inventory_file  galaxy.yml", 
            "title": "Running the playbook"
        }, 
        {
            "location": "/examples/docker/", 
            "text": "Building and deploying galaxy-kickstart in docker\n\n\n\n\nRequirements\n\n\nYou need to have docker installed and configured for your user.\n\n\nThe repository comes with various Dockerfiles that can be used to configure a deployment using Docker,\nor you can start with a pre-built docker image.\n\n\nRunning images from the dockerhub\n\n\nYou can obtain a pre-built docker image from the dockerhub:\n\n\ndocker pull artbio/galaxy-kickstart-base\n\n\n\n\nStart the image and serve it on port 8080 of your local machine in the standard docker way:\n\n\nCID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base`\n\n\n\n\n-p 8080:80\n will forward requests to nginx inside the container running on port 80.\n\n\nRuntime changes to pre-built docker images\n\n\nIf you wish to reach the container on a subdirectory, add \n-e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"\n to the docker call \nand galaxy will be served at \nhttp://127.0.0.1:8080/my-subdirectory\n.\n\n\nWe recommend changing the default admin user as well, so the command becomes:\n\n\nCID=`docker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`\n\n\n\n\nCommit changed containers to new images\n\n\nAs with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking:\nCommit the changes to my-new-image\n\n\ndocker commit $CID my-new-image\n\n\n\n\n\nStop and remove the original container:\n\n\ndocker stop $CID \n docker rm $CID\n\n\n\n\nStart the new container:\n\n\nCID=`docker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`\n\n\n\n\nPersisting to disk\n\n\nAll changes made to the container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nAssuming you would like to mount your local \n/data\n folder, run\n\n\nCID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image`\n\n\n\n\nThis will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Docker"
        }, 
        {
            "location": "/examples/docker/#building-and-deploying-galaxy-kickstart-in-docker", 
            "text": "", 
            "title": "Building and deploying galaxy-kickstart in docker"
        }, 
        {
            "location": "/examples/docker/#requirements", 
            "text": "You need to have docker installed and configured for your user.  The repository comes with various Dockerfiles that can be used to configure a deployment using Docker,\nor you can start with a pre-built docker image.", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/docker/#running-images-from-the-dockerhub", 
            "text": "You can obtain a pre-built docker image from the dockerhub:  docker pull artbio/galaxy-kickstart-base  Start the image and serve it on port 8080 of your local machine in the standard docker way:  CID=`docker run -d -p 8080:80 artbio/galaxy-kickstart-base`  -p 8080:80  will forward requests to nginx inside the container running on port 80.", 
            "title": "Running images from the dockerhub"
        }, 
        {
            "location": "/examples/docker/#runtime-changes-to-pre-built-docker-images", 
            "text": "If you wish to reach the container on a subdirectory, add  -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"  to the docker call \nand galaxy will be served at  http://127.0.0.1:8080/my-subdirectory .  We recommend changing the default admin user as well, so the command becomes:  CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 artbio/galaxy-kickstart-base`", 
            "title": "Runtime changes to pre-built docker images"
        }, 
        {
            "location": "/examples/docker/#commit-changed-containers-to-new-images", 
            "text": "As with standard docker containers, you can change, tag and commit running containers when you have configured them to your liking:\nCommit the changes to my-new-image  docker commit $CID my-new-image  Stop and remove the original container:  docker stop $CID   docker rm $CID  Start the new container:  CID=`docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 8080:80 my-new-image`", 
            "title": "Commit changed containers to new images"
        }, 
        {
            "location": "/examples/docker/#persisting-to-disk", 
            "text": "All changes made to the container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nAssuming you would like to mount your local  /data  folder, run  CID=`docker run -d --privileged -v /data:/export -p 8080:80 my-new-image`  This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Persisting to disk"
        }, 
        {
            "location": "/examples/vagrant/", 
            "text": "Deploying galaxy-kickstart on local virtual machine (VM) using vagrant.\n\n\n\n\nGalaxyKickstarter is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box\nthat runs this playbook. Following these instructions will not change the host system.\nAlternatively, see \nexamples/docker\n for running the playbook in docker,\nor \ngetting started\n for running the playbook on local or remote machines.\n\n\nRequirements\n\n\nTo follow the examples \nansible\n, \nvagrant\n\nand \ngit\n need to be installed.\n\n\nRunning the playbook on a Virtual Machine\n\n\nThe Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty).\n\n\nVAGRANTFILE_API_VERSION = \n2\n\n   Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n      config.vm.box = \nubuntu/trusty64\n\n      config.vm.network \nforwarded_port\n, guest: 80, host: 8080\n      config.vm.network \nforwarded_port\n, guest: 21, host: 2121\n\n      config.vm.provider \nvirtualbox\n do |v|\n         v.memory = 4096\n      end\n\n      config.vm.provision \nansible\n do |ansible|\n         ansible.extra_vars = {\n            ntp_server: \npool.ntp.org\n,\n            ansible_ssh_user: 'vagrant'\n         }\n         ansible.verbose = 'vvvv'\n         ansible.playbook = \ngalaxy.yml\n\n      end\n   end\n\n\n\n\nBy default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP),\nand 4096 MB of memory will be attributed to the VM.\nEnter the playbook directory \ncd ansible-artimed\n and type \nvagrant up\n to download a VM image and run the \ngalaxy.yml\n playbook.\n\n\nThis will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 .\nIf you would like to see the internals of the VM, you can log into the machine by typing \nvagrant ssh\n.\n\n\nvagrant up\n makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner\nand running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222)\nThe hosts inventory file contains an example for directly pointing ansible to the vagrant machine.\nUncomment the vagrant specific lines and comment or remove the remaining lines:\n\n\n#[artimed]\n#localhost ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n#[travis_bioblend]\n#localhost ansible_connection=local\n# Uncomment the 2 lines below to point ansible to a local vagrant machine.\n[all]\nlocalhost ansible_user=\nvagrant\n ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key\n#[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].\n\n\n\n\nTo run the playbook again, type\n\n\nansible-playbook --inventory-file=\nyour_inventory\n galaxy.yml\n\n\n\n\nCleaning up\n\n\nThe VM image and various config files have been written to the \n.vagrant\n folder. Type \nvagrant halt\n to stop the running instance\nand \nvagrant destroy\n to remove the VM, and then delete the \n.vagrant\n folder.", 
            "title": "Vagrant"
        }, 
        {
            "location": "/examples/vagrant/#deploying-galaxy-kickstart-on-local-virtual-machine-vm-using-vagrant", 
            "text": "GalaxyKickstarter is designed to be flexible and powerful, but for demonstration purposes we start a simple vagrant box\nthat runs this playbook. Following these instructions will not change the host system.\nAlternatively, see  examples/docker  for running the playbook in docker,\nor  getting started  for running the playbook on local or remote machines.", 
            "title": "Deploying galaxy-kickstart on local virtual machine (VM) using vagrant."
        }, 
        {
            "location": "/examples/vagrant/#requirements", 
            "text": "To follow the examples  ansible ,  vagrant \nand  git  need to be installed.", 
            "title": "Requirements"
        }, 
        {
            "location": "/examples/vagrant/#running-the-playbook-on-a-virtual-machine", 
            "text": "The Vagrantfile describes a Virtual Machine (VM) that is based on Ubuntu 14.04 (codename trusty).  VAGRANTFILE_API_VERSION =  2 \n   Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|\n      config.vm.box =  ubuntu/trusty64 \n      config.vm.network  forwarded_port , guest: 80, host: 8080\n      config.vm.network  forwarded_port , guest: 21, host: 2121\n\n      config.vm.provider  virtualbox  do |v|\n         v.memory = 4096\n      end\n\n      config.vm.provision  ansible  do |ansible|\n         ansible.extra_vars = {\n            ntp_server:  pool.ntp.org ,\n            ansible_ssh_user: 'vagrant'\n         }\n         ansible.verbose = 'vvvv'\n         ansible.playbook =  galaxy.yml \n      end\n   end  By default, port 8080 will be forwarded to port 80, and port 2121 will be forwarded to port 21 (for FTP),\nand 4096 MB of memory will be attributed to the VM.\nEnter the playbook directory  cd ansible-artimed  and type  vagrant up  to download a VM image and run the  galaxy.yml  playbook.  This will take a while. Once finished, you should find a running Galaxy Instance on http://localhost:8080 .\nIf you would like to see the internals of the VM, you can log into the machine by typing  vagrant ssh .  vagrant up  makes use of the ansible provisioner and is equivalent of starting a vagrant machine without the ansible provisioner\nand running ansible through an ssh connection to the vagrant machine (which listens by default on port 2222)\nThe hosts inventory file contains an example for directly pointing ansible to the vagrant machine.\nUncomment the vagrant specific lines and comment or remove the remaining lines:  #[artimed]\n#localhost ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n#[travis_bioblend]\n#localhost ansible_connection=local\n# Uncomment the 2 lines below to point ansible to a local vagrant machine.\n[all]\nlocalhost ansible_user= vagrant  ansible_port=2222 ansible_private_key_file=.vagrant/machines/default/virtualbox/private_key\n#[aws]\n# Put you aws IP and key here to make FTP work in the default VPC.\n# If you want further group-specific variables, put the host in these groups as well [e.g artimed].  To run the playbook again, type  ansible-playbook --inventory-file= your_inventory  galaxy.yml", 
            "title": "Running the playbook on a Virtual Machine"
        }, 
        {
            "location": "/examples/vagrant/#cleaning-up", 
            "text": "The VM image and various config files have been written to the  .vagrant  folder. Type  vagrant halt  to stop the running instance\nand  vagrant destroy  to remove the VM, and then delete the  .vagrant  folder.", 
            "title": "Cleaning up"
        }, 
        {
            "location": "/faq/", 
            "text": "What is the username and password of the galaxy admin account ?\n\n\nUsername and password of the galaxy account are controlled by the variables \ngalaxy_admin\n and \ngalaxy_admin_pw\n and\ndefault to \nadmin@galaxy.org\n and \nadmin\n (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on.\nIf you have a host in the \nmygroup\n group, you can edit group_vars/my_group and set\n\n\ngalaxy_admin: new_admin@email.com\ngalaxy_admin_pw: new_password\n\n\n\n\nAs with each change, run the playbook again.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/faq/#what-is-the-username-and-password-of-the-galaxy-admin-account", 
            "text": "Username and password of the galaxy account are controlled by the variables  galaxy_admin  and  galaxy_admin_pw  and\ndefault to  admin@galaxy.org  and  admin  (Defaults are defined in group_vars/all). This should be changed in the group or host variables for the host you are working on.\nIf you have a host in the  mygroup  group, you can edit group_vars/my_group and set  galaxy_admin: new_admin@email.com\ngalaxy_admin_pw: new_password  As with each change, run the playbook again.", 
            "title": "What is the username and password of the galaxy admin account ?"
        }, 
        {
            "location": "/available_roles/", 
            "text": "ansible-postgresql-objects\n\n\ngalaxy-extras role\n\n\ngalaxy-tools role\n\n\ngalaxy-os role\n\n\ngalaxy role", 
            "title": "Available roles"
        }, 
        {
            "location": "/available_variables/", 
            "text": "", 
            "title": "Available variables"
        }, 
        {
            "location": "/editing_help/", 
            "text": "To view the help, type\n\n\nmkdocs serve\n\n\n\n\nThis will serve the documentation on http://127.0.0.1:8000.\n\n\nTo add new files, create a markdown document in docs/, and reference it in mkdocs.yml.\n\nIf you want to add a document called \nediting_help.md\n add the following line.\n\n\n - Editing the readme: editing_help.md\n\n\n\n\nso that \nmkdocs.yml\n looks like this.\n\n\nsite_name: GalaxyKickstarter\npages:\n - Home: index.md\n - What is GalaxyKickstarter: about.md\n - Getting started: getting_started.md\n - Customizations: customizations.md\n - Examples: examples.md\n - Available roles: available_roles.md\n - Available variables: available_variables.md\n - Editing the readme: editing_help.md\ntheme: readthedocs", 
            "title": "Editing the readme"
        }, 
        {
            "location": "/about_metavisitor/", 
            "text": "Metavisitor\n\n\nMetavisitor\n is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules.\n\n\nHere we provide documentation on how to install and use a Galaxy server instance with pre-installed Metavisitor tools and workflows.\n\n\n\n\nUsing GalaxyKickstarter: see \nMetavisitor with GalaxyKickstarter (Ansible)\n\n\nUsing Docker: see \nMetavitor with Docker\n\n\n\n\nMetavisitor has been developed at the \nARTbio platform\n its tools and workflows are available in [GitHub] (https://github.com/ARTbio/tools-artbio) and in the [Galaxy main tool shed] (https://toolshed.g2.bx.psu.edu/).", 
            "title": "About metavisitor"
        }, 
        {
            "location": "/about_metavisitor/#metavisitor", 
            "text": "Metavisitor  is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules.  Here we provide documentation on how to install and use a Galaxy server instance with pre-installed Metavisitor tools and workflows.   Using GalaxyKickstarter: see  Metavisitor with GalaxyKickstarter (Ansible)  Using Docker: see  Metavitor with Docker   Metavisitor has been developed at the  ARTbio platform  its tools and workflows are available in [GitHub] (https://github.com/ARTbio/tools-artbio) and in the [Galaxy main tool shed] (https://toolshed.g2.bx.psu.edu/).", 
            "title": "Metavisitor"
        }, 
        {
            "location": "/metavisitor_ansible/", 
            "text": "Installing Metavisitor with GalaxyKickstarter and Ansible\n\n\n\n\nHere, a \nDeployment Machine\n will install a Metavisitor Galaxy server on \nTarget Machine\n. Note that \nDeployment Machine\n and \nTarget Machine\n can both be local or remote machines, and that they can be the same machine.\n\n\nRequirements\n\n\n\n\n\n\nOn the \nDeployment Machine\n, \ngit\n and \nansible\n need to be installed.\n\n\nThe \nTarget Machine\n has to be accessible through ssh connection by the user (you) with \nroot\n privileges. This implies that a correct ssh private key file is available on your \nDeployment Machine\n, for instance \n~/.ssh/id_rsa\n. This key will be used for secure transactions managed by ansible between the \nDeployment Machine\n and the \nTarget Machine\n.\n\n\n\n\nGetting the ansible playbook\n\n\n\n\nThis is done on the \nDeployment Machine\n by cloning the \nGalaxyKickstarter (ansible-artimed) repository\n hosted by \nthe ARTbio organization\n:\n\n\nIn your terminal, type:\n\n\ngit clone --recursive https://github.com/ARTbio/ansible-artimed.git\n\n\n\n\nImportantly, GalaxyKickstarter makes use of submodules, so care\nneeds to be taken to also download these submodules. This is why \n--recursive\n is included in the git command line.\n\n\nAt completion of the git cloning, you will have a new \nansible-artimed\n folder, which contains everything need for deployment with ansible, including the playbook file (here \ngalaxy.yml\n). You can verify this by typing in terminal:\n\n\nls -la ansible-artimed\n\n\nAdapting the ansible-artimed folder to your deployment\n\n\n\n\nThere are only few things to change in the \nansible-artimed\n folder before running ansible.\n\n\nAdapt the ansible inventory file\n\n\nIn the ansible-artimed folder, there is a \nhosts\n file called the \"inventory file\".\nFor deploying Metavisitor, you need to edit this file so that it just contains\n\n\n\n[metavisitor]\n\n\nip address\n ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\npath/to/the/ssh/private/key\n\n\n\n\n\n\nThe \nip address\n is the address of the \nTarget Machine\n. The \npath/to/the/ssh/private/key\n is the path \non the \nDeployment Machine\n to your ssh key, to be recognized by the \nTarget Machine\n.\n\n\nThus, a practical exemple of the final content on the inventory file \nhosts\n is:\n\n\n\n[metavisitor]\n\n192.54.201.126 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n\n\n\n\n\nwhere \n192.54.201.126\n is the ip address of the \nTarget machine\n and \n~/.ssh/id_rsa\n the path to the private ssh key.\n\n\nAdapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine\n\n\nIn this specific case, add in the hosts inventory file:\n\n\n[metavisitor]\n192.54.201.126 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/aws_private_key.pem\n\n\n[aws]\n192.54.201.126\n\n\n\n\nIn that case \naws_private_key.pem\n is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud.\n\n\nNote also that port range 49152 - 65534 should be open for the AWS instance in order to allow ftp upload (set this range in the security group associated to the AWS instance)\n\n\nAdapt the group_vars/all file for persisting data, if needed.\n\n\nIn cases where your \nTarget machine\n has volumes where you wish the Galaxy data to be persisted in, you have to edit the \nansible-artimed/group_vars/all\n file, to indicate the path to this volume on the \nTarget machine\n.\n\n\nIf you don't understand the previous statement, no worries, just don't do anything and skip this step.\n\n\nFor others, find the lines\n\n\n#persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export\n\n\n\n\nin the \nansible-artimed/group_vars/all\n file, and change /export to the path of your persistent volume.\n\n\nNote that if \n/export\n is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the \n/home/galaxy/galaxy\n folder of the \nTarget Machine\n.\n\n\nDeploying Metavisitor Galaxy on the \nTarget Machine\n\n\n\n\nYou are almost done.\n\n\nNavigate with your terminal to your \nansible-artimed\n folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the \nTarget Machine\n:\n\n\nansible-playbook --inventory-file=hosts galaxy.yml\n\n\n\n\nIf everything is ok, you may be asked to authorize the access to the \nTarget Machine\n by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the \nTarget Machine\n in this terminal.\n\n\nWhen the process is finished, you should be able to access the \nTarget Machine\n by typing its IP address in your web browser.\n\n\nBy default the admin login/password is \nadmin@galaxy.org\n / \nadmin\n. You should change the password for safety.\n\n\nRe-deploying Metavisitor Galaxy on the \nTarget Machine\n\n\n\n\nIf you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickstarter by editing the content of \nansible-artimed\n.\n\n\nIn that case, when your changes are done, just run again the command\n\n\nansible-playbook --inventory-file=hosts galaxy.yml\n\n\n\n\nWhen you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped.\nWhenever you change a variable (see \ncustomizations\n), you need to run the playbook again.\n\n\nYou can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses:\n\n\n[metavisitor]\n192.54.201.126 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n192.54.201.127 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa\n\n192.54.201.128 ansible_ssh_user=\nroot\n ansible_ssh_private_key_file=\n~/.ssh/id_rsa", 
            "title": "Installing with GalaxyKickstarter"
        }, 
        {
            "location": "/metavisitor_ansible/#installing-metavisitor-with-galaxykickstarter-and-ansible", 
            "text": "Here, a  Deployment Machine  will install a Metavisitor Galaxy server on  Target Machine . Note that  Deployment Machine  and  Target Machine  can both be local or remote machines, and that they can be the same machine.", 
            "title": "Installing Metavisitor with GalaxyKickstarter and Ansible"
        }, 
        {
            "location": "/metavisitor_ansible/#requirements", 
            "text": "On the  Deployment Machine ,  git  and  ansible  need to be installed.  The  Target Machine  has to be accessible through ssh connection by the user (you) with  root  privileges. This implies that a correct ssh private key file is available on your  Deployment Machine , for instance  ~/.ssh/id_rsa . This key will be used for secure transactions managed by ansible between the  Deployment Machine  and the  Target Machine .", 
            "title": "Requirements"
        }, 
        {
            "location": "/metavisitor_ansible/#getting-the-ansible-playbook", 
            "text": "This is done on the  Deployment Machine  by cloning the  GalaxyKickstarter (ansible-artimed) repository  hosted by  the ARTbio organization :  In your terminal, type:  git clone --recursive https://github.com/ARTbio/ansible-artimed.git  Importantly, GalaxyKickstarter makes use of submodules, so care\nneeds to be taken to also download these submodules. This is why  --recursive  is included in the git command line.  At completion of the git cloning, you will have a new  ansible-artimed  folder, which contains everything need for deployment with ansible, including the playbook file (here  galaxy.yml ). You can verify this by typing in terminal:  ls -la ansible-artimed", 
            "title": "Getting the ansible playbook"
        }, 
        {
            "location": "/metavisitor_ansible/#adapting-the-ansible-artimed-folder-to-your-deployment", 
            "text": "There are only few things to change in the  ansible-artimed  folder before running ansible.", 
            "title": "Adapting the ansible-artimed folder to your deployment"
        }, 
        {
            "location": "/metavisitor_ansible/#adapt-the-ansible-inventory-file", 
            "text": "In the ansible-artimed folder, there is a  hosts  file called the \"inventory file\".\nFor deploying Metavisitor, you need to edit this file so that it just contains  \n[metavisitor] ip address  ansible_ssh_user= root  ansible_ssh_private_key_file= path/to/the/ssh/private/key   The  ip address  is the address of the  Target Machine . The  path/to/the/ssh/private/key  is the path  on the  Deployment Machine  to your ssh key, to be recognized by the  Target Machine .  Thus, a practical exemple of the final content on the inventory file  hosts  is:  \n[metavisitor]\n\n192.54.201.126 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa   where  192.54.201.126  is the ip address of the  Target machine  and  ~/.ssh/id_rsa  the path to the private ssh key.", 
            "title": "Adapt the ansible inventory file"
        }, 
        {
            "location": "/metavisitor_ansible/#adapt-the-ansible-inventory-file-to-an-amazon-web-service-aws-virtual-machine", 
            "text": "In this specific case, add in the hosts inventory file:  [metavisitor]\n192.54.201.126 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/aws_private_key.pem \n\n[aws]\n192.54.201.126  In that case  aws_private_key.pem  is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud.  Note also that port range 49152 - 65534 should be open for the AWS instance in order to allow ftp upload (set this range in the security group associated to the AWS instance)", 
            "title": "Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine"
        }, 
        {
            "location": "/metavisitor_ansible/#adapt-the-group_varsall-file-for-persisting-data-if-needed", 
            "text": "In cases where your  Target machine  has volumes where you wish the Galaxy data to be persisted in, you have to edit the  ansible-artimed/group_vars/all  file, to indicate the path to this volume on the  Target machine .  If you don't understand the previous statement, no worries, just don't do anything and skip this step.  For others, find the lines  #persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export  in the  ansible-artimed/group_vars/all  file, and change /export to the path of your persistent volume.  Note that if  /export  is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the  /home/galaxy/galaxy  folder of the  Target Machine .", 
            "title": "Adapt the group_vars/all file for persisting data, if needed."
        }, 
        {
            "location": "/metavisitor_ansible/#deploying-metavisitor-galaxy-on-the-target-machine", 
            "text": "You are almost done.  Navigate with your terminal to your  ansible-artimed  folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the  Target Machine :  ansible-playbook --inventory-file=hosts galaxy.yml  If everything is ok, you may be asked to authorize the access to the  Target Machine  by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the  Target Machine  in this terminal.  When the process is finished, you should be able to access the  Target Machine  by typing its IP address in your web browser.  By default the admin login/password is  admin@galaxy.org  /  admin . You should change the password for safety.", 
            "title": "Deploying Metavisitor Galaxy on the Target Machine"
        }, 
        {
            "location": "/metavisitor_ansible/#re-deploying-metavisitor-galaxy-on-the-target-machine", 
            "text": "If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickstarter by editing the content of  ansible-artimed .  In that case, when your changes are done, just run again the command  ansible-playbook --inventory-file=hosts galaxy.yml  When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped.\nWhenever you change a variable (see  customizations ), you need to run the playbook again.  You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses:  [metavisitor]\n192.54.201.126 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n192.54.201.127 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa \n192.54.201.128 ansible_ssh_user= root  ansible_ssh_private_key_file= ~/.ssh/id_rsa", 
            "title": "Re-deploying Metavisitor Galaxy on the Target Machine"
        }, 
        {
            "location": "/metavisitor_docker/", 
            "text": "Installing Metavisitor with Docker\n\n\n\n\nWe distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".\n\n\nRequirements\n\n\nYou need to have docker installed and configured for your user.\n\n\nRunning images from the dockerhub\n\n\nYou can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container:\n\n\ndocker search metavisitor\n\n\n\n\nThen, to get the docker image, type:\n\n\ndocker pull artbio/metavisitor-1.2\n\n\n\n\nIn this documentation, we recommend to use the \nartbio/metavisitor-1.2\n which better corresponds to the environment described in our \nMetavisitor preprint\n\n\nWhen this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing:\n\n\ndocker run -d -p 80:80 artbio/metavisitor-1.2\n\n\n\n\nThis command starts a container in daemon mode (\n-d\n) from the image and serve it on port 80 of the local machine in the standard docker way.\n\n\n-p 80:80\n forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change \n-p 80:80\n to \n-p 8080:80\n\n\nRuntime changes to pre-built docker images\n\n\nIf you wish to reach the container on a subdirectory, add \n-e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"\n to the docker call.\n\n\nFor instance,\n\n\ndocker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -p 80:80 artbio/metavisitor-1.2\n\n\n\n\nwill get the metavisitor docker container serving at \nhttp://127.0.0.1:80/my-subdirectory\n.\n\n\nWe recommend also changing the default admin user as well, so the command becomes:\n\n\ndocker run -d -e NGINX_GALAXY_LOCATION=\n/my-subdirectory\n -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base\n\n\n\n\nNote that is you do not make this latest change, the admin login for the metavisitor container is by default \nadmin@galaxy.org\n and the password is \nadmin\n.\n\n\nPersisting to disk\n\n\nAll changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nThus, assuming you would like to mount your local \n/my/data\n folder and persist you Galaxy data in this folder, run\n\n\ndocker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-1.2\n\n\n\n\nThis will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Installing with Docker"
        }, 
        {
            "location": "/metavisitor_docker/#installing-metavisitor-with-docker", 
            "text": "We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".", 
            "title": "Installing Metavisitor with Docker"
        }, 
        {
            "location": "/metavisitor_docker/#requirements", 
            "text": "You need to have docker installed and configured for your user.", 
            "title": "Requirements"
        }, 
        {
            "location": "/metavisitor_docker/#running-images-from-the-dockerhub", 
            "text": "You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container:  docker search metavisitor  Then, to get the docker image, type:  docker pull artbio/metavisitor-1.2  In this documentation, we recommend to use the  artbio/metavisitor-1.2  which better corresponds to the environment described in our  Metavisitor preprint  When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing:  docker run -d -p 80:80 artbio/metavisitor-1.2  This command starts a container in daemon mode ( -d ) from the image and serve it on port 80 of the local machine in the standard docker way.  -p 80:80  forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change  -p 80:80  to  -p 8080:80", 
            "title": "Running images from the dockerhub"
        }, 
        {
            "location": "/metavisitor_docker/#runtime-changes-to-pre-built-docker-images", 
            "text": "If you wish to reach the container on a subdirectory, add  -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"  to the docker call.  For instance,  docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -p 80:80 artbio/metavisitor-1.2  will get the metavisitor docker container serving at  http://127.0.0.1:80/my-subdirectory .  We recommend also changing the default admin user as well, so the command becomes:  docker run -d -e NGINX_GALAXY_LOCATION= /my-subdirectory  -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base  Note that is you do not make this latest change, the admin login for the metavisitor container is by default  admin@galaxy.org  and the password is  admin .", 
            "title": "Runtime changes to pre-built docker images"
        }, 
        {
            "location": "/metavisitor_docker/#persisting-to-disk", 
            "text": "All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone.\nTo persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into\nthe containers /export folder.\nDue to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container.\nThus, assuming you would like to mount your local  /my/data  folder and persist you Galaxy data in this folder, run  docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-1.2  This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data).\nFrom the new location the files are being bind-mounted back into their original location.", 
            "title": "Persisting to disk"
        }
    ]
}